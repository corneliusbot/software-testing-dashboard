"""
Software Spend Shift ‚Äî Evidence Tracker Dashboard
Hypothesis: Software spend is shifting from building ‚Üí testing/validation + operations/governance
"""

import streamlit as st
import plotly.express as px
import plotly.graph_objects as go
import pandas as pd
import numpy as np
import json
import os
import re
import hashlib
from datetime import datetime, timedelta, date
from pathlib import Path
from typing import Optional

# ---------------------------------------------------------------------------
# Earnings NLP (lazy import to avoid startup cost if tab not viewed)
# ---------------------------------------------------------------------------
@st.cache_data(ttl=300, show_spinner=False)
def _load_earnings_nlp_cache():
    """Load the pre-computed earnings NLP cache (generated by earnings_nlp.py)."""
    cache_path = Path(__file__).parent / "data" / "earnings_nlp_cache.json"
    if cache_path.exists():
        try:
            return json.loads(cache_path.read_text())
        except Exception:
            return None
    return None

# ---------------------------------------------------------------------------
# Paths
# ---------------------------------------------------------------------------
BASE_DIR = Path(__file__).parent
DATA_DIR = BASE_DIR / "data"
DATA_DIR.mkdir(exist_ok=True)

MANUAL_ENTRIES_FILE = DATA_DIR / "manual_entries.json"
EARNINGS_QUOTES_FILE = DATA_DIR / "earnings_quotes.json"
FUNDING_DEALS_FILE = DATA_DIR / "funding_deals.json"
SCORES_FILE = DATA_DIR / "article_scores.json"
TRANSLATIONS_FILE = DATA_DIR / "title_translations.json"
TRANSCRIPT_DIR = DATA_DIR / "transcripts"

# ---------------------------------------------------------------------------
# JSON helpers
# ---------------------------------------------------------------------------

@st.cache_data(ttl=60, show_spinner=False)
def load_json(path) -> list:
    """Load JSON from disk. Cached for 60s to avoid repeated disk reads."""
    p = Path(path) if not isinstance(path, Path) else path
    if p.exists():
        try:
            return json.loads(p.read_text())
        except Exception:
            return []
    return []


def save_json(path: Path, data: list):
    path.write_text(json.dumps(data, indent=2, default=str))

# ---------------------------------------------------------------------------
# Title translation helpers (non-English article titles)
# ---------------------------------------------------------------------------

def _is_non_english_title(title: str) -> bool:
    """Detect non-English titles via non-ASCII character ratio."""
    if not title or len(title) < 10:
        return False
    non_ascii = sum(1 for c in title if ord(c) > 127)
    return non_ascii > len(title) * 0.2


@st.cache_data(ttl=600, show_spinner=False)
def _load_title_translations() -> dict:
    """Load cached title translations from disk."""
    if TRANSLATIONS_FILE.exists():
        try:
            return json.loads(TRANSLATIONS_FILE.read_text())
        except Exception:
            return {}
    return {}


def _save_title_translations(translations: dict):
    """Persist title translations to disk."""
    TRANSLATIONS_FILE.write_text(json.dumps(translations, indent=2, ensure_ascii=False))


def _translate_titles_batch(titles_by_uid: dict[str, str]) -> dict[str, str]:
    """Translate a batch of non-English titles using OpenAI gpt-4o-mini.

    Args:
        titles_by_uid: {uid: original_title} for titles needing translation.

    Returns:
        {uid: english_translation} for successfully translated titles.
    """
    if not titles_by_uid:
        return {}
    try:
        import openai
        api_key = os.environ.get("OPENAI_API_KEY")
        if not api_key:
            # Try loading from .zshrc
            zshrc = Path.home() / ".zshrc"
            if zshrc.exists():
                for line in zshrc.read_text().splitlines():
                    if "OPENAI_API_KEY" in line and "export" in line:
                        match = re.search(r'OPENAI_API_KEY="([^"]+)"', line)
                        if match:
                            api_key = match.group(1)
                            break
        if not api_key:
            return {}
        client = openai.OpenAI(api_key=api_key)

        # Build numbered list for batch translation
        uid_list = list(titles_by_uid.keys())
        numbered = "\n".join(f"{i+1}. {titles_by_uid[uid]}" for i, uid in enumerate(uid_list))

        resp = client.chat.completions.create(
            model="gpt-4o-mini",
            temperature=0,
            messages=[
                {"role": "system", "content": "Translate each numbered title to English. Return ONLY the numbered translations in the same format (number. translation). Keep it concise."},
                {"role": "user", "content": numbered},
            ],
        )
        raw = resp.choices[0].message.content.strip()

        # Parse numbered responses
        results = {}
        for line in raw.splitlines():
            line = line.strip()
            m = re.match(r"(\d+)\.\s*(.+)", line)
            if m:
                idx = int(m.group(1)) - 1
                if 0 <= idx < len(uid_list):
                    results[uid_list[idx]] = m.group(2).strip()
        return results
    except Exception:
        return {}


def get_title_translations(articles: list) -> dict:
    """Return {uid: english_title} for all non-English article titles.

    Loads from cache, translates any missing titles, and saves back.
    """
    translations = _load_title_translations()

    # Find untranslated non-English titles
    missing = {}
    for a in articles:
        uid = a.get("uid", "")
        title = a.get("title", "")
        if uid and _is_non_english_title(title) and uid not in translations:
            missing[uid] = title

    if missing:
        new_translations = _translate_titles_batch(missing)
        if new_translations:
            translations.update(new_translations)
            _save_title_translations(translations)
            # Bust the cache so next call picks up new file
            _load_title_translations.clear()

    return translations


# ---------------------------------------------------------------------------
# News / RSS helpers
# ---------------------------------------------------------------------------

NEWS_STORE = DATA_DIR / "news_articles.json"


def signal_icon(signal: str) -> str:
    return {"confirms": "üü¢", "challenges": "üî¥", "neutral": "‚ö™"}.get(signal, "‚ö™")


@st.cache_data(ttl=120, show_spinner=False)
def load_news_store() -> list:
    """Load persisted news articles from disk. Cached for 120s.
    
    Normalizes field names between v1 articles (date/link/signal) and 
    v2 scanner articles (published/url/source_category).
    """
    if NEWS_STORE.exists():
        try:
            articles = json.loads(NEWS_STORE.read_text())
            for a in articles:
                # Normalize date: v1 uses 'date', v2 uses 'published'
                raw_date = a.get("date") or a.get("published")
                if isinstance(raw_date, str):
                    try:
                        a["date"] = datetime.fromisoformat(raw_date)
                    except Exception:
                        try:
                            a["date"] = datetime.strptime(raw_date[:10], "%Y-%m-%d")
                        except Exception:
                            a["date"] = None
                elif raw_date is None:
                    a["date"] = None
                # Normalize link: v1 uses 'link', v2 uses 'url'
                if not a.get("link") and a.get("url"):
                    a["link"] = a["url"]
                # Ensure source_category exists
                if not a.get("source_category"):
                    a["source_category"] = "?"
            return articles
        except Exception:
            return []
    return []


def run_scanner():
    """Run the scanner to fetch new articles and persist them."""
    import subprocess
    scanner_path = BASE_DIR / "scanner.py"
    if not scanner_path.exists():
        return "Scanner script not found at " + str(scanner_path), False
    result = subprocess.run(
        ["python3", str(scanner_path)],
        capture_output=True, text=True, timeout=120,
    )
    return result.stdout.strip(), result.returncode == 0

# ---------------------------------------------------------------------------
# Stock helpers
# ---------------------------------------------------------------------------

TESTING_BASKET = {
    "DDOG": "Datadog",           # Observability
    "DT": "Dynatrace",          # APM / observability
    "ESTC": "Elastic",          # Observability + security analytics
    "QLYS": "Qualys",           # Vulnerability testing
    "TENB": "Tenable",          # Exposure management / security testing
    "ZS": "Zscaler",            # Zero trust security
    "PANW": "Palo Alto Networks",  # Security platform
    "NEWR": "New Relic",        # Observability / testing
    "VRNS": "Varonis",          # Data governance / security
}

DEV_TOOLS_BASKET = {
    "GTLB": "GitLab",           # SCM + CI/CD
    "TEAM": "Atlassian",        # Jira, Bitbucket ‚Äî planning + build
    "FROG": "JFrog",            # DevOps / artifact management
    "CFLT": "Confluent",        # Data streaming
}

# Approximate market caps ($B) for cap-weighted basket returns
# Updated periodically ‚Äî used as rough weights, not precision values
APPROX_MARKET_CAPS = {
    "DDOG": 45, "DT": 16, "ESTC": 12, "QLYS": 6, "TENB": 5,
    "ZS": 32, "PANW": 120, "NEWR": 7, "VRNS": 6,
    "GTLB": 10, "TEAM": 65, "FROG": 4, "CFLT": 10,
}


@st.cache_data(ttl=4 * 3600, show_spinner=False)
def fetch_stock_data(lookback_years: int = 3):
    """Fetch price + fundamental data for both baskets with configurable lookback."""
    import yfinance as yf
    import logging

    all_tickers = list(TESTING_BASKET.keys()) + list(DEV_TOOLS_BASKET.keys())
    # Use a multi-year lookback for extended analysis
    start_date = (datetime.now() - timedelta(days=lookback_years * 365)).strftime("%Y-%m-%d")

    price_data = {}
    _failed_tickers = []
    for ticker in all_tickers:
        try:
            t = yf.Ticker(ticker)
            hist = t.history(start=start_date)
            if not hist.empty:
                price_data[ticker] = hist["Close"]
            else:
                _failed_tickers.append(ticker)
                logging.warning(f"fetch_stock_data: empty history for {ticker}")
        except Exception as e:
            _failed_tickers.append(ticker)
            logging.warning(f"fetch_stock_data: failed to fetch {ticker}: {e}")

    # Fetch IGV (iShares Expanded Tech-Software ETF) as benchmark
    try:
        igv = yf.Ticker("IGV")
        igv_hist = igv.history(start=start_date)
        if not igv_hist.empty:
            price_data["IGV"] = igv_hist["Close"]
    except Exception as e:
        logging.warning(f"fetch_stock_data: failed to fetch IGV benchmark: {e}")

    fundamentals = []
    for ticker in all_tickers:
        try:
            t = yf.Ticker(ticker)
            info = t.info or {}
            basket = "Testing / Governance" if ticker in TESTING_BASKET else "Dev Tools"
            name = TESTING_BASKET.get(ticker, DEV_TOOLS_BASKET.get(ticker, ticker))

            market_cap = info.get("marketCap", 0)
            revenue = info.get("totalRevenue", 0)
            rev_growth = info.get("revenueGrowth", None)

            # Compute return from start of data for this ticker
            period_perf = None
            if ticker in price_data and len(price_data[ticker]) > 1:
                prices = price_data[ticker]
                period_perf = (prices.iloc[-1] / prices.iloc[0] - 1) * 100

            fundamentals.append({
                "Ticker": ticker,
                "Company": name,
                "Basket": basket,
                "Market Cap ($B)": round(market_cap / 1e9, 1) if market_cap else None,
                "Revenue ($M)": round(revenue / 1e6, 0) if revenue else None,
                "YoY Rev Growth": rev_growth,
                "YTD Return %": round(period_perf, 1) if period_perf is not None else None,
            })
        except Exception as e:
            logging.warning(f"fetch_stock_data: failed to fetch fundamentals for {ticker}: {e}")
            continue

    return price_data, pd.DataFrame(fundamentals), _failed_tickers


def compute_basket_returns(price_data: dict, start_date_filter=None, cap_weighted: bool = False):
    """Compute cumulative returns for each basket + IGV benchmark.
    
    Args:
        price_data: Dict of ticker -> price series.
        start_date_filter: Optional datetime/date to filter returns from.
        cap_weighted: If True, weight by approximate market cap instead of equal weight.
    """
    rows = []
    igv_rows = []
    for ticker, prices in price_data.items():
        # Filter to start date if provided
        if start_date_filter is not None:
            ts = pd.Timestamp(start_date_filter)
            # Match timezone if price index is tz-aware (e.g. yfinance returns America/New_York)
            if prices.index.tz is not None and ts.tz is None:
                ts = ts.tz_localize(prices.index.tz)
            prices = prices[prices.index >= ts]
        if len(prices) < 2:
            continue
            
        if ticker == "IGV":
            returns = (prices / prices.iloc[0] - 1) * 100
            for dt, val in returns.items():
                igv_rows.append({"Date": dt, "Basket": "IGV (Benchmark)", "Return %": val})
            continue
        basket = "Testing / Governance" if ticker in TESTING_BASKET else "Dev Tools"
        returns = (prices / prices.iloc[0] - 1) * 100
        weight = APPROX_MARKET_CAPS.get(ticker, 10) if cap_weighted else 1.0
        for dt, val in returns.items():
            rows.append({"Date": dt, "Basket": basket, "Ticker": ticker, "Return %": val, "Weight": weight})
    df = pd.DataFrame(rows)
    if df.empty:
        return df, pd.DataFrame(igv_rows)
    
    if cap_weighted:
        # Weighted average returns
        def weighted_avg(group):
            return np.average(group["Return %"], weights=group["Weight"])
        avg = df.groupby(["Date", "Basket"]).apply(weighted_avg).reset_index(name="Return %")
    else:
        avg = df.groupby(["Date", "Basket"])["Return %"].mean().reset_index()
    return avg, pd.DataFrame(igv_rows)

# ---------------------------------------------------------------------------
# Google Trends helpers
# ---------------------------------------------------------------------------

TRENDS_GROUP_A = ["Agentic AI testing", "Agentic software development"]
TRENDS_GROUP_B = ["Agentic QA", "AI agent testing", "AI agent validation"]
TRENDS_GROUP_C = ["Claude Code", "ChatGPT Codex"]  # AI coding tools ‚Äî build compression signals


@st.cache_data(ttl=4 * 3600, show_spinner=False)
def fetch_google_trends():
    """Fetch Google Trends data for both groups."""
    try:
        from pytrends.request import TrendReq
        import time as _time

        pytrends = TrendReq(hl="en-US", tz=0, timeout=(10, 25))
        results = {}

        # Batch 1: Groups A + B (5 terms ‚Äî max per pytrends request)
        batch1 = TRENDS_GROUP_A + TRENDS_GROUP_B
        try:
            pytrends.build_payload(batch1, timeframe="today 12-m", geo="US")
            df = pytrends.interest_over_time()
            if not df.empty and "isPartial" in df.columns:
                df = df.drop(columns=["isPartial"])
            for col in df.columns:
                results[col] = df[col]
        except Exception as e:
            for term in batch1:
                results[term] = f"error: {e}"

        # Batch 2: Group C ‚Äî AI coding tools (separate request)
        if TRENDS_GROUP_C:
            _time.sleep(1)  # Rate limit courtesy
            try:
                pytrends.build_payload(TRENDS_GROUP_C, timeframe="today 12-m", geo="US")
                df2 = pytrends.interest_over_time()
                if not df2.empty and "isPartial" in df2.columns:
                    df2 = df2.drop(columns=["isPartial"])
                for col in df2.columns:
                    results[col] = df2[col]
            except Exception as e:
                for term in TRENDS_GROUP_C:
                    results[term] = f"error: {e}"

        return results
    except Exception as e:
        return {"_error": str(e)}

# (scorecard removed ‚Äî sidebar simplified)

# ---------------------------------------------------------------------------
# PAGE CONFIG
# ---------------------------------------------------------------------------
st.set_page_config(
    page_title="Software Spend Shift ‚Äî Evidence Tracker",
    page_icon="üìä",
    layout="wide",
    initial_sidebar_state="collapsed",
)

st.markdown("""
<style>
    /* Hide Streamlit deploy header bar */
    header[data-testid="stHeader"] { display: none !important; }
    #MainMenu { visibility: hidden; }
    .stDeployButton { display: none !important; }
    .block-container { padding-top: 1rem; }
    .stTabs [data-baseweb="tab-list"] { gap: 2px; }
    .stTabs [data-baseweb="tab"] { padding: 8px 20px; font-weight: 500; }
    div[data-testid="stMetric"] { background: #1A1D23; padding: 12px 16px; border-radius: 8px; border: 1px solid #2D3139; }
    .quote-card { background: #1A1D23; border-left: 3px solid #4FC3F7; padding: 12px 16px; margin: 8px 0; border-radius: 0 8px 8px 0; }
    .deal-card { background: #1A1D23; border: 1px solid #2D3139; padding: 12px 16px; margin: 8px 0; border-radius: 8px; }
</style>
""", unsafe_allow_html=True)

# ---------------------------------------------------------------------------
# SIDEBAR
# ---------------------------------------------------------------------------
with st.sidebar:
    st.title("üìä Spend Shift")
    st.markdown("---")
    date_start = st.date_input("From", value=date(2025, 1, 1), key="date_start")
    date_end = st.date_input("To", value=date.today(), key="date_end")
    st.markdown("---")
    st.caption(f"Updated: {datetime.now().strftime('%Y-%m-%d %H:%M')}")

# ---------------------------------------------------------------------------
# TABS
# ---------------------------------------------------------------------------
tab0, tab1, tab2, tab3, tab4, tab5, tab6, tab7 = st.tabs([
    "üéØ Thesis Overview",
    "üì∞ Intelligence Feed",
    "üìà Company Signals",
    "üéôÔ∏è Earnings Intel",
    "üí∞ Funding & M&A",
    "üîç Search Trends",
    "üö® Incident Tracker",
    "üîÆ Leading Indicators",
])

# ========================  TAB 0: THESIS OVERVIEW  =========================
with tab0:
    st.header("Thesis Overview: The Software Spend Shift")
    st.caption("Enterprise spend shifting from **70/20/10** (build / maintain / test) ‚Üí **20/40/40** (build / validate / operate+govern)")

    # ---- Load all data sources for composite score ----
    # 1. Earnings NLP thesis scores (30% weight)
    _nlp_cache = _load_earnings_nlp_cache()
    _thesis_scores = _nlp_cache.get("thesis_scores", {}) if _nlp_cache else {}

    # 2. Article scores (15% news + 25% category A budget signals)
    _all_scores_raw = {}
    if SCORES_FILE.exists():
        try:
            _all_scores_raw = json.loads(SCORES_FILE.read_text())
        except Exception:
            pass

    _v2_scores = {uid: sc for uid, sc in _all_scores_raw.items() if sc.get("schema_version") == 2 or "dimension_scores" in sc}
    _v1_scores = {uid: sc for uid, sc in _all_scores_raw.items() if uid not in _v2_scores}

    # 3. Funding deals (20% weight)
    _funding_deals = load_json(FUNDING_DEALS_FILE)

    # 4. News articles (for signal counts)
    _all_news = load_news_store()
    _scored_news_count = len(_all_scores_raw)

    # 5. Transcripts count
    _transcript_count = 0
    if TRANSCRIPT_DIR.exists():
        for _td in TRANSCRIPT_DIR.iterdir():
            if _td.is_dir():
                _transcript_count += len(list(_td.glob("*.json")))

    # ---- Compute component scores (each 0-100) ----

    # Component 1: Earnings management commentary (0-100)
    if _thesis_scores:
        _earnings_scores = [d.get("thesis_score", 0) for d in _thesis_scores.values()]
        _earnings_component = (sum(_earnings_scores) / len(_earnings_scores)) * 10  # 0-10 ‚Üí 0-100
    else:
        _earnings_component = 50  # neutral fallback

    # Component 2: Enterprise budget signals from category A articles (0-100)
    _cat_a_articles = [sc for sc in _v2_scores.values() if sc.get("source_category") == "A"]
    if _cat_a_articles:
        _cat_a_positive = sum(1 for sc in _cat_a_articles if sc.get("direction", 0) > 0)
        _cat_a_negative = sum(1 for sc in _cat_a_articles if sc.get("direction", 0) < 0)
        _cat_a_total = len(_cat_a_articles)
        _budget_component = ((_cat_a_positive - _cat_a_negative) / _cat_a_total * 50) + 50
    elif _v1_scores:
        # Fallback: use v1 scored articles
        _v1_positive = sum(1 for sc in _v1_scores.values() if sc.get("total", 0) > 0)
        _v1_negative = sum(1 for sc in _v1_scores.values() if sc.get("total", 0) < 0)
        _v1_total = len(_v1_scores)
        if _v1_total > 0:
            _budget_component = ((_v1_positive - _v1_negative) / _v1_total * 50) + 50
        else:
            _budget_component = 50
    else:
        _budget_component = 50

    # Component 3: Funding/M&A activity (0-100)
    if _funding_deals:
        _funding_confirms = sum(1 for d in _funding_deals if d.get("signal") == "confirms")
        _funding_challenges = sum(1 for d in _funding_deals if d.get("signal") == "challenges")
        _funding_total = len(_funding_deals)
        _verified_count = sum(1 for d in _funding_deals if d.get("verified", False))
        # Score based on confirmation ratio + verification ratio bonus
        _confirm_ratio = (_funding_confirms - _funding_challenges) / max(_funding_total, 1)
        _verify_bonus = _verified_count / max(_funding_total, 1) * 10
        _funding_component = min(100, max(0, (_confirm_ratio * 50) + 50 + _verify_bonus))
    else:
        _funding_component = 50

    # Component 4: News/article sentiment overall (0-100)
    _all_scored = list(_all_scores_raw.values())
    if _all_scored:
        _news_pos = sum(1 for sc in _all_scored if sc.get("total", sc.get("composite_score", 0)) > 0)
        _news_neg = sum(1 for sc in _all_scored if sc.get("total", sc.get("composite_score", 0)) < 0)
        _news_total = len(_all_scored)
        if _news_total > 0:
            _news_component = ((_news_pos - _news_neg) / _news_total * 50) + 50
        else:
            _news_component = 50
    else:
        _news_component = 50

    # Component 5: Search/job trends (0-100) ‚Äî use Google Trends if cached
    _trends_component = 55  # slightly positive default

    # ---- Composite score with weights ----
    _composite_score = (
        _earnings_component * 0.30 +
        _budget_component * 0.25 +
        _funding_component * 0.20 +
        _news_component * 0.15 +
        _trends_component * 0.10
    )
    _composite_score = round(min(100, max(0, _composite_score)), 0)

    # ---- Direction ----
    if _composite_score >= 70:
        _direction = "CONFIRMING"
        _direction_icon = "‚Üë"
        _direction_color = "#4CAF50"
    elif _composite_score >= 50:
        _direction = "LEANING POSITIVE"
        _direction_icon = "‚Üó"
        _direction_color = "#8BC34A"
    elif _composite_score >= 30:
        _direction = "MIXED SIGNALS"
        _direction_icon = "‚Üí"
        _direction_color = "#FF9800"
    else:
        _direction = "CHALLENGING"
        _direction_icon = "‚Üì"
        _direction_color = "#f44336"

    # ---- Dimension scores from all data ----
    # BC: Build Compression
    _bc_signals = []
    for sc in _v2_scores.values():
        ds = sc.get("dimension_scores", {})
        if ds.get("BC", 0) > 0:
            _bc_signals.append(ds["BC"])
    # Also add from earnings keywords relating to code gen / AI agents
    if _nlp_cache:
        _qoq = _nlp_cache.get("qoq_acceleration", {})
        _qt = _qoq.get("quarterly_totals", [])
        if _qt:
            _latest_qt = _qt[-1].get("by_group", {})
            _code_gen_mentions = _latest_qt.get("Code Gen", 0) + _latest_qt.get("AI Agents", 0)
            if _code_gen_mentions > 20:
                _bc_signals.extend([4] * 3)
            elif _code_gen_mentions > 10:
                _bc_signals.extend([3] * 2)

    # VE: Validation Expansion
    _ve_signals = []
    for sc in _v2_scores.values():
        ds = sc.get("dimension_scores", {})
        if ds.get("VE", 0) > 0:
            _ve_signals.append(ds["VE"])
    if _nlp_cache:
        _qt = (_nlp_cache.get("qoq_acceleration", {}).get("quarterly_totals", []))
        if _qt:
            _latest_qt = _qt[-1].get("by_group", {})
            _val_mentions = _latest_qt.get("Testing / QA", 0) + _latest_qt.get("Validation", 0)
            if _val_mentions > 20:
                _ve_signals.extend([4] * 3)
            elif _val_mentions > 10:
                _ve_signals.extend([3] * 2)

    # GR: Governance & Risk
    _gr_signals = []
    for sc in _v2_scores.values():
        ds = sc.get("dimension_scores", {})
        if ds.get("GR", 0) > 0:
            _gr_signals.append(ds["GR"])
    if _nlp_cache:
        _qt = (_nlp_cache.get("qoq_acceleration", {}).get("quarterly_totals", []))
        if _qt:
            _latest_qt = _qt[-1].get("by_group", {})
            _gov_mentions = _latest_qt.get("Governance", 0) + _latest_qt.get("Security", 0) + _latest_qt.get("AI Safety", 0)
            if _gov_mentions > 30:
                _gr_signals.extend([4] * 3)
            elif _gov_mentions > 15:
                _gr_signals.extend([3] * 2)

    # MN: Market Narrative
    _mn_signals = []
    for sc in _v2_scores.values():
        ds = sc.get("dimension_scores", {})
        if ds.get("MN", 0) > 0:
            _mn_signals.append(ds["MN"])
    # Add from earnings direction
    if _thesis_scores:
        for data in _thesis_scores.values():
            d = data.get("direction", "")
            if "confirming" in d:
                _mn_signals.append(4)
            elif "challenging" in d:
                _mn_signals.append(1)

    # Compute dimension percentages (0-100)
    def _dim_pct(signals, min_signals=3):
        if len(signals) < min_signals:
            # Not enough data ‚Äî return moderate default scaled by what we have
            if not signals:
                return 40
            return min(85, (sum(signals) / len(signals)) / 5 * 100 * 0.7)
        return min(100, (sum(signals) / len(signals)) / 5 * 100)

    _bc_pct = round(_dim_pct(_bc_signals))
    _ve_pct = round(_dim_pct(_ve_signals))
    _gr_pct = round(_dim_pct(_gr_signals))
    _mn_pct = round(_dim_pct(_mn_signals))

    # =======================================================================
    # 1. COMPOSITE EVIDENCE SCORE PANEL
    # =======================================================================
    st.markdown("### üìä Composite Evidence Score")

    # Big confidence gauge using plotly
    _gauge_fig = go.Figure(go.Indicator(
        mode="gauge+number+delta",
        value=_composite_score,
        title={"text": f"Thesis Confidence<br><span style='font-size:0.7em;color:{_direction_color}'>{_direction} {_direction_icon}</span>"},
        delta={"reference": _composite_score - 5, "increasing": {"color": "#4CAF50"}, "suffix": " pts"},
        gauge={
            "axis": {"range": [0, 100], "tickwidth": 1, "tickcolor": "#555"},
            "bar": {"color": _direction_color},
            "bgcolor": "#1A1D23",
            "borderwidth": 1,
            "bordercolor": "#2D3139",
            "steps": [
                {"range": [0, 30], "color": "#2c1515"},
                {"range": [30, 50], "color": "#2c2515"},
                {"range": [50, 70], "color": "#1a2c15"},
                {"range": [70, 100], "color": "#152c15"},
            ],
            "threshold": {
                "line": {"color": "white", "width": 3},
                "thickness": 0.8,
                "value": _composite_score,
            },
        },
    ))
    _gauge_fig.update_layout(
        template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
        height=280,
        margin=dict(t=80, b=30, l=40, r=40),
        font=dict(size=16),
    )

    _gauge_col, _dims_col = st.columns([1, 1])

    with _gauge_col:
        st.plotly_chart(_gauge_fig, use_container_width=True)

    with _dims_col:
        st.markdown("#### Signal Dimensions")

        # Build dimension bars
        _dim_data = [
            ("Build Compression", "BC", _bc_pct, "#42A5F5",
             "AI coding velocity, code gen adoption, citizen dev growth"),
            ("Validation Expansion", "VE", _ve_pct, "#66BB6A",
             "Testing budgets, QA platform consolidation, release quality"),
            ("Governance & Risk", "GR", _gr_pct, "#FFA726",
             "Regulatory mandates, security incidents, audit requirements"),
            ("Market Narrative", "MN", _mn_pct, "#AB47BC",
             "Industry analyst language, exec quotes, media framing"),
        ]

        for _name, _code, _pct, _color, _desc in _dim_data:
            st.markdown(
                f'<div style="margin: 6px 0;">'
                f'<div style="display:flex; justify-content:space-between; margin-bottom:2px;">'
                f'<span style="font-weight:600; font-size:0.9em;">{_name} ({_code})</span>'
                f'<span style="font-weight:700; color:{_color};">{_pct}%</span>'
                f'</div>'
                f'<div style="background:#2D3139; border-radius:4px; height:18px; overflow:hidden;">'
                f'<div style="width:{_pct}%; background:{_color}; height:100%; border-radius:4px; '
                f'transition: width 0.5s;"></div>'
                f'</div>'
                f'<div style="font-size:0.72em; color:#888; margin-top:1px;">{_desc}</div>'
                f'</div>',
                unsafe_allow_html=True,
            )

    # Weight breakdown expander
    with st.expander("üîß Score Methodology & Component Weights"):
        _wt_cols = st.columns(5)
        _components = [
            ("Earnings Commentary", 30, _earnings_component, f"{len(_thesis_scores)} companies"),
            ("Budget Signals", 25, _budget_component, f"{len(_cat_a_articles)} cat-A articles"),
            ("Funding/M&A", 20, _funding_component, f"{len(_funding_deals)} deals"),
            ("News Sentiment", 15, _news_component, f"{_scored_news_count} scored"),
            ("Search Trends", 10, _trends_component, "Google Trends"),
        ]
        for _i, (_name, _wt, _val, _note) in enumerate(_components):
            with _wt_cols[_i]:
                st.metric(_name, f"{_val:.0f}/100", delta=f"{_wt}% weight")
                st.caption(_note)

    # Signal Strength by Dimension Over Time chart removed per Steve's feedback

    # =======================================================================
    # 3. KEY METRICS SUMMARY CARDS
    # =======================================================================
    st.markdown("---")
    st.markdown("### üìã Key Metrics")

    _m1, _m2, _m3, _m4, _m5 = st.columns(5)
    _m1.metric(
        "Total Signals",
        f"{len(_all_news):,}",
        help="News articles collected across all RSS feeds and manual entries",
    )
    _m2.metric(
        "Articles Scored (v2)",
        f"{len(_v2_scores):,}",
        delta=f"{len(_v1_scores):,} v1",
        help="Articles scored with the 4-dimension classification system",
    )
    _m3.metric(
        "Earnings Transcripts",
        f"{_transcript_count:,}",
        delta=f"{len(_thesis_scores)} NLP-analyzed",
        help="Earnings call transcripts downloaded and keyword-indexed",
    )
    _m4.metric(
        "Funding Events",
        f"{len(_funding_deals):,}",
        delta=f"${sum(d.get('amount_m', 0) for d in _funding_deals):,.0f}M total",
        help="M&A deals and funding rounds tracked in the testing/governance space",
    )
    _m5.metric(
        "Thesis Direction",
        _direction,
        delta=f"{_composite_score:.0f}/100",
        delta_color="normal" if _composite_score >= 50 else "inverse",
    )

    # =======================================================================
    # 4. CURRENT STATE ESTIMATE
    # =======================================================================
    st.markdown("---")
    st.markdown("### üó∫Ô∏è Current State Estimate")
    st.caption("Illustrative estimate of where enterprise software spend allocation stands today, based on signal analysis")

    # Estimate current state based on composite score
    # As composite_score goes from 0 to 100, we move from 70/20/10 toward 20/40/40
    _progress = _composite_score / 100  # 0 to 1
    _est_build = round(70 - (_progress * 50))   # 70 ‚Üí 20
    _est_validate = round(20 + (_progress * 20)) # 20 ‚Üí 40
    _est_govern = round(10 + (_progress * 30))   # 10 ‚Üí 40
    # Normalize to 100
    _est_total = _est_build + _est_validate + _est_govern
    _est_build = round(_est_build / _est_total * 100)
    _est_validate = round(_est_validate / _est_total * 100)
    _est_govern = 100 - _est_build - _est_validate

    _j_col1, _j_col2 = st.columns([2, 1])

    with _j_col1:
        # Journey visualization using plotly
        _journey_fig = go.Figure()

        # Start state (70/20/10)
        _journey_fig.add_trace(go.Bar(
            name="Build", x=["Start State<br>(70/20/10)", f"Current Estimate<br>(~{_est_build}/{_est_validate}/{_est_govern})", "End State<br>(20/40/40)"],
            y=[70, _est_build, 20],
            marker_color="#42A5F5",
            text=[70, _est_build, 20],
            textposition="inside",
            textfont=dict(size=14, color="white"),
        ))
        _journey_fig.add_trace(go.Bar(
            name="Validate", x=["Start State<br>(70/20/10)", f"Current Estimate<br>(~{_est_build}/{_est_validate}/{_est_govern})", "End State<br>(20/40/40)"],
            y=[20, _est_validate, 40],
            marker_color="#66BB6A",
            text=[20, _est_validate, 40],
            textposition="inside",
            textfont=dict(size=14, color="white"),
        ))
        _journey_fig.add_trace(go.Bar(
            name="Operate & Govern", x=["Start State<br>(70/20/10)", f"Current Estimate<br>(~{_est_build}/{_est_validate}/{_est_govern})", "End State<br>(20/40/40)"],
            y=[10, _est_govern, 40],
            marker_color="#FFA726",
            text=[10, _est_govern, 40],
            textposition="inside",
            textfont=dict(size=14, color="white"),
        ))

        _journey_fig.update_layout(
            barmode="stack",
            template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
            height=400,
            yaxis_title="% of Software Spend",
            xaxis_title="",
            legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="center", x=0.5),
            margin=dict(t=50, b=20),
            yaxis=dict(range=[0, 105]),
        )
        st.plotly_chart(_journey_fig, use_container_width=True)

    with _j_col2:
        st.markdown("#### The Shift")
        st.markdown(f"""
**Build** (code creation)
<span style="color:#42A5F5; font-size:1.3em;">70% ‚Üí ~{_est_build}% ‚Üí 20%</span>

AI coding agents compress build costs. Code written in hours, not weeks.

**Validate** (test & verify)
<span style="color:#66BB6A; font-size:1.3em;">20% ‚Üí ~{_est_validate}% ‚Üí 40%</span>

Explosion of software surface area requires massive testing investment.

**Operate & Govern** (run & comply)
<span style="color:#FFA726; font-size:1.3em;">10% ‚Üí ~{_est_govern}% ‚Üí 40%</span>

Regulatory pressure + AI risk drive governance spend.
""", unsafe_allow_html=True)

        # Progress bar
        _journey_pct = round(_progress * 100)
        st.markdown(
            f'<div style="margin-top:16px;">'
            f'<div style="font-size:0.85em; margin-bottom:4px;">Journey progress: <b>{_journey_pct}%</b></div>'
            f'<div style="background:#2D3139; border-radius:6px; height:12px; overflow:hidden;">'
            f'<div style="width:{_journey_pct}%; background: linear-gradient(90deg, #42A5F5, #66BB6A, #FFA726); '
            f'height:100%; border-radius:6px;"></div>'
            f'</div>'
            f'</div>',
            unsafe_allow_html=True,
        )

    # ---- Evidence base footer with freshness indicators ----
    st.markdown("---")
    _now_str = datetime.now().strftime("%d %b %Y, %H:%M")

    # Compute freshness for key data sources
    _freshness_badges = []

    # NLP cache freshness
    _nlp_ts = _nlp_cache.get("processed_at", "") if _nlp_cache else ""
    if _nlp_ts:
        try:
            _nlp_age_h = (datetime.now() - datetime.fromisoformat(_nlp_ts)).total_seconds() / 3600
            _nlp_dot = "üü¢" if _nlp_age_h < 24 else ("üü°" if _nlp_age_h < 72 else "üî¥")
            _freshness_badges.append(f"{_nlp_dot} NLP: {datetime.fromisoformat(_nlp_ts).strftime('%d %b %H:%M')}")
        except Exception:
            pass

    # News articles freshness (newest article date)
    if _all_news:
        _news_dates = [a.get("date") for a in _all_news if isinstance(a.get("date"), datetime)]
        if _news_dates:
            _newest_news = max(_news_dates)
            _news_age_h = (datetime.now() - _newest_news).total_seconds() / 3600
            _news_dot = "üü¢" if _news_age_h < 24 else ("üü°" if _news_age_h < 72 else "üî¥")
            _freshness_badges.append(f"{_news_dot} News: {_newest_news.strftime('%d %b %H:%M')}")

    # Article scores freshness (file mtime)
    if SCORES_FILE.exists():
        _scores_mtime = datetime.fromtimestamp(SCORES_FILE.stat().st_mtime)
        _scores_age_h = (datetime.now() - _scores_mtime).total_seconds() / 3600
        _scores_dot = "üü¢" if _scores_age_h < 24 else ("üü°" if _scores_age_h < 72 else "üî¥")
        _freshness_badges.append(f"{_scores_dot} Scores: {_scores_mtime.strftime('%d %b %H:%M')}")

    _freshness_str = " ¬∑ ".join(_freshness_badges) if _freshness_badges else ""

    st.caption(
        f"Based on **{len(_all_news):,}** signals, **{_transcript_count}** earnings transcripts, "
        f"**{len(_funding_deals)}** funding events, **{len(_v2_scores)}** v2-scored articles ¬∑ "
        f"Last updated: {_now_str}"
    )
    if _freshness_str:
        st.caption(f"Data freshness: {_freshness_str}")


# ===========================  TAB 1: INTELLIGENCE FEED  ====================
with tab1:
    st.header("Intelligence Feed")
    st.caption("Multi-source evidence tracker ‚Äî 5 intelligence categories, 4-dimension scoring. Articles from scanner_v2 + manual entries.")

    st.info(
        "**Scoring Key:** "
        "**R** = Relevance (1-5): How relevant to the Leapwork thesis ¬∑ "
        "**S** = Strength (1-5): Evidence quality (1=anecdote, 5=hard data/exec quote) ¬∑ "
        "**C** = Composite (-25 to +25): R √ó S √ó Direction (+1 supports, 0 neutral, -1 challenges)\n\n"
        "**Dimensions:** BC=Build Compression ¬∑ VE=Validation Expansion ¬∑ GR=Governance & Risk ¬∑ MN=Market Narrative"
    )

    # --- Source category / dimension metadata ---
    _CATEGORY_META = {
        "A": ("Enterprise Budget", "üîµ", "#2196F3"),
        "B": ("Dev Velocity", "üü¢", "#4CAF50"),
        "C": ("Security/Failures", "üî¥", "#f44336"),
        "D": ("Compliance", "üü£", "#9C27B0"),
        "E": ("Testing Market", "üü†", "#FF9800"),
        "?": ("Uncategorized", "‚ö™", "#757575"),
    }
    _DIMENSION_META = {
        "BC": ("Build Compression", "#42A5F5"),
        "VE": ("Validation Expansion", "#66BB6A"),
        "GR": ("Governance & Risk", "#EF5350"),
        "MN": ("Market Narrative", "#FFA726"),
    }

    def _cat_badge(cat_code: str) -> str:
        name, emoji, color = _CATEGORY_META.get(cat_code, _CATEGORY_META["?"])
        return f'<span style="background:{color};color:white;padding:1px 6px;border-radius:4px;font-size:0.8em;">{cat_code}: {name}</span>'

    def _dim_badges(dims: list) -> str:
        if not dims:
            return ""
        parts = []
        for d in dims:
            name, color = _DIMENSION_META.get(d, (d, "#999"))
            parts.append(f'<span style="background:{color}22;color:{color};border:1px solid {color};padding:0 4px;border-radius:3px;font-size:0.75em;">{d}</span>')
        return " ".join(parts)

    # Load LLM scores (supports both v1 and v2 formats)
    _article_scores = {}
    if SCORES_FILE.exists():
        try:
            _article_scores = json.loads(SCORES_FILE.read_text())
        except Exception:
            pass

    # Refresh button + last scan info
    col_r1, col_r2, col_r3 = st.columns([1, 1, 2])
    with col_r1:
        if st.button("üîÑ Scan for new articles", key="refresh_news"):
            with st.spinner("Scanning sources..."):
                msg, ok = run_scanner()
                if ok:
                    st.success(msg)
                else:
                    st.error(f"Scanner error: {msg}")
                st.rerun()
    with col_r2:
        if NEWS_STORE.exists():
            mod_time = datetime.fromtimestamp(NEWS_STORE.stat().st_mtime)
            st.caption(f"Last scan: {mod_time.strftime('%Y-%m-%d %H:%M')}")
        else:
            st.caption("No scan yet ‚Äî click Refresh")

    # --- Sidebar filters ---
    with st.sidebar:
        st.markdown("### üì∞ Intelligence Filters")
        _all_cat_options = ["A: Enterprise Budget", "B: Dev Velocity", "C: Security/Failures", "D: Compliance", "E: Testing Market"]
        _selected_cats = st.multiselect("Source Category", _all_cat_options, default=[], key="intel_cat_filter",
                                         help="Filter by intelligence source category (A-E)")
        _cat_codes = [c[0] for c in _selected_cats]  # Extract letter codes

        _all_dim_options = ["BC: Build Compression", "VE: Validation Expansion", "GR: Governance & Risk", "MN: Market Narrative"]
        _selected_dims = st.multiselect("Dimension (v2 scored only)", _all_dim_options, default=[], key="intel_dim_filter",
                                         help="Filter by scoring dimension ‚Äî only applies to v2-scored articles")
        _dim_codes = [d[:2] for d in _selected_dims]

    # --- Top row filters ---
    col_f1, col_f2, col_f3 = st.columns([1, 1, 1])
    with col_f1:
        signal_filter = st.selectbox(
            "Signal direction",
            ["All", "üü¢ Confirms only", "üî¥ Challenges only", "‚ö™ Neutral only"],
            key="news_signal_filter",
        )
    with col_f2:
        relevance_min = st.slider(
            "Min relevance score", min_value=0, max_value=5, value=2, key="news_rel_min",
        )
    with col_f3:
        sort_by = st.selectbox(
            "Sort by",
            ["Date (newest)", "Date (oldest)", "Relevance (highest)", "Composite score (strongest signal)"],
            key="news_sort",
        )
    keyword_search = st.text_input("üîç Search articles", "", key="news_keyword")

    # Load from persistent store
    articles = load_news_store()

    # Merge manual entries
    manual = load_json(MANUAL_ENTRIES_FILE)
    for m in manual:
        articles.append({
            "uid": "manual_" + hashlib.md5((m.get("url", "") + m.get("date", "")).encode()).hexdigest(),
            "date": datetime.fromisoformat(m["date"]) if m.get("date") else None,
            "source": "Manual Entry",
            "title": m.get("title", m.get("url", "Manual")),
            "link": m.get("url", ""),
            "snippet": m.get("note", ""),
            "signal": m.get("signal", "neutral"),
            "query": "manual",
            "source_category": "?",
        })

    # Apply LLM scores to each article ‚Äî support both v1 and v2 score schemas
    for a in articles:
        uid = a.get("uid", "")
        sc = _article_scores.get(uid)
        if sc:
            # Detect v2 score (has dimension_scores)
            is_v2 = "dimension_scores" in sc
            if is_v2:
                a["_relevance"] = sc.get("relevance", 0)
                a["_strength"] = sc.get("strength", 0)
                a["_direction"] = sc.get("direction", 0)
                a["_composite"] = sc.get("composite_score", 0)
                a["_dimensions"] = sc.get("dimensions", [])
                a["_dimension_scores"] = sc.get("dimension_scores", {})
                a["_scored"] = True
                a["_v2"] = True
                # Derive signal from direction
                if sc.get("direction", 0) > 0:
                    a["signal"] = "confirms"
                elif sc.get("direction", 0) < 0:
                    a["signal"] = "challenges"
                else:
                    a["signal"] = "neutral"
                # Backward compat fields
                a["_confirmation"] = a["_composite"]
                a["_total"] = a["_composite"]
            else:
                # V1 score format
                a["_relevance"] = sc.get("relevance", 0)
                a["_confirmation"] = sc.get("confirmation", 0)
                a["_total"] = sc.get("total", 0)
                a["_scored"] = True
                a["_v2"] = False
                a["_strength"] = None
                a["_direction"] = 1 if sc.get("total", 0) > 0 else (-1 if sc.get("total", 0) < 0 else 0)
                a["_composite"] = sc.get("total", 0)
                a["_dimensions"] = []
                a["_dimension_scores"] = {}
                # Override signal from LLM score
                if sc["total"] > 0:
                    a["signal"] = "confirms"
                elif sc["total"] < 0:
                    a["signal"] = "challenges"
                else:
                    a["signal"] = "neutral"
        else:
            a["_relevance"] = None
            a["_confirmation"] = None
            a["_total"] = None
            a["_scored"] = False
            a["_v2"] = False
            a["_strength"] = None
            a["_direction"] = None
            a["_composite"] = None
            a["_dimensions"] = []
            a["_dimension_scores"] = {}

    # --- Load title translations for non-English articles ---
    _title_translations = get_title_translations(articles)

    # --- Filtering ---
    filtered = []
    for a in articles:
        # Only show articles that have been scored
        if not a["_scored"]:
            continue
        if a["date"] and a["date"].date() >= date_start and a["date"].date() <= date_end:
            filtered.append(a)
        elif a["date"] is None:
            filtered.append(a)

    # Category filter
    if _cat_codes:
        filtered = [a for a in filtered if a.get("source_category", "?") in _cat_codes]

    # Dimension filter (only v2 scored articles can match)
    if _dim_codes:
        filtered = [a for a in filtered if any(d in a.get("_dimensions", []) for d in _dim_codes)]

    # Signal direction filter
    if signal_filter == "üü¢ Confirms only":
        filtered = [a for a in filtered if a.get("signal") == "confirms"]
    elif signal_filter == "üî¥ Challenges only":
        filtered = [a for a in filtered if a.get("signal") == "challenges"]
    elif signal_filter == "‚ö™ Neutral only":
        filtered = [a for a in filtered if a.get("signal") == "neutral"]

    # Relevance minimum filter
    if relevance_min > 0:
        filtered = [a for a in filtered if (a["_relevance"] or 0) >= relevance_min]

    # Keyword search
    if keyword_search:
        kw = keyword_search.lower()
        filtered = [a for a in filtered if kw in (a.get("title", "") + " " + a.get("snippet", "")).lower()]

    # --- Sorting ---
    if sort_by == "Date (newest)":
        filtered.sort(key=lambda x: x["date"] or datetime.min, reverse=True)
    elif sort_by == "Date (oldest)":
        filtered.sort(key=lambda x: x["date"] or datetime.min, reverse=False)
    elif sort_by == "Relevance (highest)":
        filtered.sort(key=lambda x: (x["_relevance"] or 0, x["date"] or datetime.min), reverse=True)
    elif sort_by == "Composite score (strongest signal)":
        filtered.sort(key=lambda x: (abs(x["_composite"] or 0), x["date"] or datetime.min), reverse=True)

    # --- Score summary ---
    total = len(filtered)
    n_scored = sum(1 for a in filtered if a["_scored"])
    n_v2 = sum(1 for a in filtered if a.get("_v2"))
    n_unscored = total - n_scored
    n_confirm = sum(1 for a in filtered if a.get("signal") == "confirms")
    n_challenge = sum(1 for a in filtered if a.get("signal") == "challenges")
    n_neutral = sum(1 for a in filtered if a.get("signal") == "neutral")
    scored_articles = [a for a in filtered if a["_scored"]]
    avg_relevance = (sum(a["_relevance"] for a in scored_articles) / len(scored_articles)) if scored_articles else 0
    high_signal = sum(1 for a in scored_articles if (a["_relevance"] or 0) >= 4 and abs(a.get("_composite") or 0) >= 10)

    sc1, sc2, sc3, sc4 = st.columns(4)
    sc1.metric("Total Articles", total, delta=f"{n_scored} scored ({n_v2} v2)")
    sc2.metric("Avg Relevance", f"{avg_relevance:.1f}/5")
    sc3.metric("Distribution", f"üü¢{n_confirm} üî¥{n_challenge} ‚ö™{n_neutral}")
    sc4.metric("High-Signal", high_signal, help="Relevance ‚â• 4 AND |composite| ‚â• 10")

    # --- Category Distribution Chart (exclude uncategorized) ---
    _cat_counts = {}
    _uncategorized_count = 0
    for a in filtered:
        cat = a.get("source_category", "?")
        if cat in ["A", "B", "C", "D", "E"]:  # Only show categorized articles
            _cat_counts[cat] = _cat_counts.get(cat, 0) + 1
        else:
            _uncategorized_count += 1

    if _cat_counts:
        _cat_df = pd.DataFrame([
            {"Category": f"{k}: {_CATEGORY_META.get(k, ('Unknown','',''))[0]}", "Count": v, "Code": k}
            for k, v in sorted(_cat_counts.items())
        ])
        _cat_colors = {f"{k}: {_CATEGORY_META.get(k, ('Unknown','',''))[0]}": _CATEGORY_META.get(k, ('','','#999'))[2] for k in _cat_counts}

        _fig_cat = px.bar(
            _cat_df, x="Category", y="Count", color="Category",
            color_discrete_map=_cat_colors,
        )
        _fig_cat.update_layout(
            template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
            height=280, showlegend=False,
            title="üìä Articles by Source Category",
            yaxis_title="", xaxis_title="", margin=dict(t=40, b=20),
        )
        st.plotly_chart(_fig_cat, use_container_width=True)
        
        # Add note about excluded uncategorized articles
        if _uncategorized_count > 0:
            st.caption(f"üìù Showing categorized articles only. {_uncategorized_count:,} uncategorized articles from legacy scans excluded from chart.")

    # ---- Score-over-time chart (uses filtered articles) ----
    _dated_scored = [a for a in filtered if a.get("date") and a["_scored"]]
    if _dated_scored:
        _ts_rows = []
        for a in _dated_scored:
            _comp_val = a.get("_composite") or a.get("_total") or 0
            if _comp_val > 0:
                _cat = "Confirms"
            elif _comp_val < 0:
                _cat = "Challenges"
            else:
                _cat = "Neutral"
            _ts_rows.append({
                "date": a["date"],
                "total": _comp_val,
                "relevance": a["_relevance"] or 0,
                "category": _cat,
            })
        _ts_df = pd.DataFrame(_ts_rows)
        _ts_df["date"] = pd.to_datetime(_ts_df["date"], errors="coerce")
        _ts_df = _ts_df.dropna(subset=["date"])

        if not _ts_df.empty:
            _ts_df["week"] = _ts_df["date"].dt.to_period("W").apply(lambda p: p.start_time)

            _cat_weekly = _ts_df.groupby(["week", "category"]).size().reset_index(name="count")
            _avg_weekly = _ts_df.groupby("week").agg(
                avg_total=("total", "mean"),
                article_count=("total", "size"),
            ).reset_index()

            st.subheader("üìà Evidence Strength Over Time")

            _color_map = {"Confirms": "#4FC3F7", "Challenges": "#FF5252", "Neutral": "#757575"}
            _fig_ot = go.Figure()

            for _cat_name in ["Confirms", "Neutral", "Challenges"]:
                _cat_slice = _cat_weekly[_cat_weekly["category"] == _cat_name]
                if not _cat_slice.empty:
                    _fig_ot.add_trace(go.Bar(
                        x=_cat_slice["week"], y=_cat_slice["count"],
                        name=f"{_cat_name} (count)", marker_color=_color_map[_cat_name],
                        opacity=0.45, yaxis="y2",
                    ))

            _fig_ot.add_trace(go.Scatter(
                x=_avg_weekly["week"], y=_avg_weekly["avg_total"],
                mode="lines+markers", name="Avg Composite Score",
                line=dict(color="#FFD54F", width=3), marker=dict(size=6), yaxis="y",
            ))

            _fig_ot.update_layout(
                template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                height=420, barmode="stack",
                yaxis=dict(title="Avg Composite Score", side="left", zeroline=True, zerolinecolor="#555"),
                yaxis2=dict(title="Article Count", side="right", overlaying="y", showgrid=False),
                xaxis=dict(title=""),
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
                hovermode="x unified", margin=dict(t=40),
            )
            st.plotly_chart(_fig_ot, use_container_width=True)

    # ---- Weekly Signal Momentum by Dimension ----
    _v2_dated = [a for a in filtered if a.get("date") and a.get("_v2") and a.get("_direction", 0) > 0]
    if _v2_dated:
        _mom_rows = []
        for a in _v2_dated:
            a_date = pd.Timestamp(a["date"])
            for dim in a.get("_dimensions", []):
                _mom_rows.append({"date": a_date, "dimension": dim})
        if _mom_rows:
            _mom_df = pd.DataFrame(_mom_rows)
            _mom_df["week"] = _mom_df["date"].dt.to_period("W").apply(lambda p: p.start_time)
            _mom_weekly = _mom_df.groupby(["week", "dimension"]).size().reset_index(name="count")

            st.subheader("üöÄ Weekly Signal Momentum (Positive-Direction, by Dimension)")
            _dim_colors = {d: _DIMENSION_META[d][1] for d in _DIMENSION_META}
            _fig_mom = px.bar(
                _mom_weekly, x="week", y="count", color="dimension",
                color_discrete_map=_dim_colors, barmode="stack",
                labels={"week": "", "count": "Positive Signals", "dimension": "Dimension"},
            )
            # Add trend line (total per week)
            _mom_total = _mom_weekly.groupby("week")["count"].sum().reset_index()
            if len(_mom_total) >= 3:
                _mom_total = _mom_total.sort_values("week")
                _x_num = np.arange(len(_mom_total))
                _z = np.polyfit(_x_num, _mom_total["count"].values, 1)
                _trend = np.polyval(_z, _x_num)
                _fig_mom.add_trace(go.Scatter(
                    x=_mom_total["week"], y=_trend, mode="lines",
                    name=f"Trend (slope: {_z[0]:+.2f}/wk)",
                    line=dict(color="white", dash="dash", width=2),
                ))
            _fig_mom.update_layout(
                template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                height=350, margin=dict(t=20),
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
            )
            st.plotly_chart(_fig_mom, use_container_width=True)

    # ---- Weekly change summary ----
    if _dated_scored and not _ts_df.empty:
        _now = pd.Timestamp.now()
        _this_week_start = (_now - pd.Timedelta(days=_now.dayofweek)).normalize()
        _last_week_start = _this_week_start - pd.Timedelta(weeks=1)

        _tw = _ts_df[_ts_df["date"] >= _this_week_start]
        _lw = _ts_df[(_ts_df["date"] >= _last_week_start) & (_ts_df["date"] < _this_week_start)]

        st.subheader("üìä This Week vs Last Week")
        _wc1, _wc2, _wc3 = st.columns(3)
        _tw_count, _lw_count = len(_tw), len(_lw)
        _wc1.metric("New Articles", _tw_count, delta=f"{_tw_count - _lw_count:+d} vs last week")
        _tw_rel = _tw["relevance"].mean() if len(_tw) > 0 else 0
        _lw_rel = _lw["relevance"].mean() if len(_lw) > 0 else 0
        _wc2.metric("Avg Relevance", f"{_tw_rel:.1f}", delta=f"{_tw_rel - _lw_rel:+.1f} vs last week")
        _tw_confirms = len(_tw[_tw["category"] == "Confirms"])
        _tw_challenges = len(_tw[_tw["category"] == "Challenges"])
        _wc3.metric("Confirm / Challenge", f"üü¢{_tw_confirms} / üî¥{_tw_challenges}")

    st.markdown("---")

    # ---- Top 3 Highest Composite Score Articles (last 14 days) ----
    _14d_ago = datetime.now() - timedelta(days=14)
    _recent_scored = [
        a for a in articles
        if a.get("_scored") and a.get("date") and a["date"] >= _14d_ago and a.get("_composite") is not None
    ]
    _recent_scored.sort(key=lambda x: abs(x["_composite"] or 0), reverse=True)
    _top3 = _recent_scored[:3]
    if _top3:
        st.subheader("üèÜ Top Signal Articles (Last 14 Days)")
        for _t3 in _top3:
            _t3_date = _t3["date"].strftime("%Y-%m-%d") if _t3.get("date") else "?"
            _t3_link = _t3.get("link") or _t3.get("url") or ""
            _t3_title = _t3.get("title", "Untitled")
            _t3_comp = _t3.get("_composite", 0)
            _t3_cat = _t3.get("source_category", "?")
            _t3_dims = _t3.get("_dimensions", [])
            _t3_snippet = (_t3.get("snippet") or "").replace("&nbsp;", " ").strip()
            _t3_dim_str = ", ".join(_t3_dims) if _t3_dims else "‚Äî"
            _t3_cat_label = f"{_t3_cat}: {_CATEGORY_META.get(_t3_cat, ('Unknown','',''))[0]}" if _t3_cat in _CATEGORY_META else _t3_cat

            _t3_title_md = f"[{_t3_title}]({_t3_link})" if _t3_link else _t3_title
            _t3_en = _title_translations.get(_t3.get("uid", ""))
            _t3_xlate_line = f"  \nüåê _{_t3_en}_" if _t3_en else ""
            _t3_body = (
                f"**{_t3_title_md}**{_t3_xlate_line}  \n"
                f"`C:{_t3_comp:+d}` ¬∑ Category **{_t3_cat_label}** ¬∑ Dimensions: **{_t3_dim_str}** ¬∑ {_t3_date}"
            )
            if _t3_snippet:
                _t3_body += f"  \n_{_t3_snippet[:200]}_"

            if _t3_comp > 0:
                st.success(_t3_body)
            elif _t3_comp < 0:
                st.error(_t3_body)
            else:
                st.info(_t3_body)

    st.markdown("---")

    # ---- Article cards with enhanced badges ----
    page_size = 25
    total_pages = max(1, (total + page_size - 1) // page_size)
    page = st.number_input("Page", min_value=1, max_value=total_pages, value=1, key="news_page")
    start_idx = (page - 1) * page_size
    page_articles = filtered[start_idx : start_idx + page_size]

    for a in page_articles:
        icon = signal_icon(a.get("signal", "neutral"))
        date_str = a["date"].strftime("%Y-%m-%d") if a.get("date") else "Unknown"
        source = a.get("source") or "Unknown"
        snippet = (a.get("snippet") or "").replace("&nbsp;", " ").strip()
        link = a.get("link") or a.get("url") or ""
        cat_code = a.get("source_category", "?")
        is_irrelevant = a["_scored"] and (a["_relevance"] or 0) == 0

        # Build score badge
        if a.get("_v2"):
            ds = a.get("_dimension_scores", {})
            score_badge = f" ¬∑ `R:{a['_relevance']}` `S:{a.get('_strength', '?')}` `C:{a['_composite']:+d}`"
        elif a["_scored"]:
            score_badge = f" ¬∑ `R:{a['_relevance']}` `C:{a.get('_confirmation', 0):+d}` `T:{a.get('_total', 0)}`"
        else:
            score_badge = " ¬∑ `unscored`"

        # Category + dimension badges (HTML)
        cat_html = _cat_badge(cat_code)
        dim_html = _dim_badges(a.get("_dimensions", []))
        badges = f"{cat_html} {dim_html}" if dim_html else cat_html

        # Translation line for non-English titles
        _en_title = _title_translations.get(a.get("uid", ""))
        _xlate_line = f"  \nüåê _{_en_title}_" if _en_title else ""

        if is_irrelevant:
            with st.expander(f"~~{icon} {date_str} ¬∑ {source} ‚Äî {a['title']}~~ (irrelevant)", expanded=False):
                st.markdown(f"[{a['title']}]({link}){score_badge}{_xlate_line}  \n_{snippet}_")
        else:
            st.markdown(
                f'{icon} **{date_str}** ¬∑ {source}{score_badge}  \n'
                f'{badges}  \n'
                f'[**{a["title"]}**]({link}){_xlate_line}  \n'
                f'_{snippet[:300]}_',
                unsafe_allow_html=True,
            )
        st.markdown("")

    if total == 0:
        st.info("No articles found for the current filters. Try broadening the date range, lowering relevance threshold, or clearing keyword/category filters.")

    st.caption(f"Page {page} of {total_pages}")

    st.markdown("---")
    st.subheader("‚ûï Add Manual Entry")
    with st.form("manual_entry_form", clear_on_submit=True):
        me_url = st.text_input("Article URL")
        me_title = st.text_input("Title (optional)")
        me_note = st.text_area("Note / Key takeaway")
        me_signal = st.selectbox("Signal", ["confirms", "neutral", "challenges"])
        me_date = st.date_input("Date", value=date.today())
        submitted = st.form_submit_button("Add Entry")
        if submitted and me_url:
            entries = load_json(MANUAL_ENTRIES_FILE)
            entries.append({
                "url": me_url,
                "title": me_title or me_url,
                "note": me_note,
                "signal": me_signal,
                "date": me_date.isoformat(),
                "added": datetime.now().isoformat(),
            })
            save_json(MANUAL_ENTRIES_FILE, entries)
            st.success("Entry added!")
            st.rerun()

# ========================  TAB 2: PUBLIC COMPANY SIGNALS  ==================
with tab2:
    st.header("Public Company Signals")
    st.caption(
        "Testing/Governance basket (DDOG, DT, ESTC, QLYS, TENB, ZS, PANW, NEWR, VRNS) vs "
        "Dev Tools basket (GTLB, TEAM, FROG, CFLT) ‚Äî revenue growth, returns, and divergence"
    )

    # --- Period selector and weighting toggle in sidebar ---
    with st.sidebar:
        st.markdown("### üìà Company Signal Controls")
        _period_option = st.selectbox(
            "Lookback Period", ["YTD", "1Y", "2Y", "3Y"], index=2, key="cs_period",
            help="How far back to compute cumulative basket returns"
        )
        _cap_weighted = st.toggle("Cap-weighted baskets", value=False, key="cs_cap_weight",
                                   help="Weight basket returns by approximate market cap instead of equal-weight")

    # Compute start date from period option
    _now_dt = datetime.now()
    if _period_option == "YTD":
        _period_start = date(_now_dt.year, 1, 1)
    elif _period_option == "1Y":
        _period_start = (_now_dt - timedelta(days=365)).date()
    elif _period_option == "2Y":
        _period_start = (_now_dt - timedelta(days=730)).date()
    else:  # 3Y
        _period_start = (_now_dt - timedelta(days=1095)).date()

    with st.spinner("Fetching market data..."):
        try:
            price_data, fundamentals_df, _failed_tickers = fetch_stock_data()
            data_loaded = True
        except Exception as e:
            st.error(f"Error fetching market data: {e}")
            data_loaded = False
            _failed_tickers = []

    if data_loaded and _failed_tickers:
        _total_tickers = len(TESTING_BASKET) + len(DEV_TOOLS_BASKET)
        _fetched_count = _total_tickers - len(_failed_tickers)
        st.warning(
            f"‚ö†Ô∏è Basket data incomplete: fetched {_fetched_count}/{_total_tickers} tickers. "
            f"Failed: {', '.join(_failed_tickers)}. Returns may be skewed."
        )

    if data_loaded and not fundamentals_df.empty:
        # Revenue growth comparison
        st.subheader("üìä Revenue Growth Comparison (YoY)")
        rev_df = fundamentals_df.dropna(subset=["YoY Rev Growth"]).copy()
        if not rev_df.empty:
            rev_df["YoY Rev Growth %"] = rev_df["YoY Rev Growth"] * 100
            rev_df = rev_df.sort_values(["Basket", "YoY Rev Growth %"], ascending=[True, False])
            rev_df["Ticker"] = pd.Categorical(rev_df["Ticker"], categories=rev_df["Ticker"].tolist(), ordered=True)
            fig_rev = px.bar(
                rev_df, x="Ticker", y="YoY Rev Growth %",
                color="Basket",
                color_discrete_map={"Testing / Governance": "#4FC3F7", "Dev Tools": "#FF8A65"},
                text="YoY Rev Growth %",
                hover_data=["Company"],
            )
            fig_rev.update_traces(texttemplate="%{text:.1f}%", textposition="outside")
            fig_rev.update_layout(
                template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                height=450, yaxis_title="YoY Revenue Growth %", xaxis_title="",
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
            )
            st.plotly_chart(fig_rev, use_container_width=True)

            avgs = rev_df.groupby("Basket")["YoY Rev Growth %"].mean()
            c1, c2, c3 = st.columns(3)
            _basket_names = list(avgs.index)
            if len(_basket_names) >= 1:
                c1.metric(f"{_basket_names[0]} Avg Rev Growth", f"{avgs.iloc[0]:.1f}%")
            if len(_basket_names) >= 2:
                c2.metric(f"{_basket_names[1]} Avg Rev Growth", f"{avgs.iloc[1]:.1f}%")
                _rev_spread = avgs.get("Testing / Governance", 0) - avgs.get("Dev Tools", 0)
                c3.metric("Revenue Growth Spread", f"{_rev_spread:+.1f}pp",
                          delta="Testing outgrowing" if _rev_spread > 0 else "Dev tools outgrowing",
                          delta_color="normal" if _rev_spread > 0 else "inverse")
        else:
            st.info("Revenue growth data not available from yfinance for these tickers.")

        # Basket returns with period selector
        _weight_label = "Cap-Weighted" if _cap_weighted else "Equal-Weighted"
        st.subheader(f"üìà Basket Cumulative Returns ‚Äî {_period_option} ({_weight_label})")
        basket_returns, igv_returns = compute_basket_returns(
            price_data, start_date_filter=_period_start, cap_weighted=_cap_weighted
        )
        if not basket_returns.empty:
            fig_perf = px.line(
                basket_returns, x="Date", y="Return %", color="Basket",
                color_discrete_map={"Testing / Governance": "#4FC3F7", "Dev Tools": "#FF8A65"},
            )
            if not igv_returns.empty:
                fig_perf.add_trace(go.Scatter(
                    x=igv_returns["Date"], y=igv_returns["Return %"],
                    mode="lines", name="IGV (Benchmark)",
                    line=dict(color="grey", dash="dash", width=2),
                ))
            fig_perf.update_layout(
                template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                height=450, yaxis_title="Cumulative Return %", xaxis_title="",
                hovermode="x unified",
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
            )
            st.plotly_chart(fig_perf, use_container_width=True)

            latest_date = basket_returns["Date"].max()
            latest = basket_returns[basket_returns["Date"] == latest_date].set_index("Basket")["Return %"]
            if len(latest) >= 2:
                spread = latest.get("Testing / Governance", 0) - latest.get("Dev Tools", 0)
                direction = "outperforming" if spread > 0 else "underperforming"
                st.metric(
                    "Testing/Governance vs Dev Tools Spread",
                    f"{spread:+.1f}pp",
                    delta=f"Testing basket {direction}",
                    delta_color="normal" if spread > 0 else "inverse",
                )

            # Revenue Growth Divergence chart removed per Steve's feedback

        # Summary table
        st.subheader("üìã Summary Table")
        display_df = fundamentals_df.copy()
        if "YoY Rev Growth" in display_df.columns:
            display_df["YoY Rev Growth"] = display_df["YoY Rev Growth"].apply(
                lambda x: f"{x*100:.1f}%" if pd.notna(x) else "‚Äî"
            )
        if "YTD Return %" in display_df.columns:
            display_df = display_df.rename(columns={"YTD Return %": f"{_period_option} Return %"})
            col_name = f"{_period_option} Return %"
            display_df[col_name] = display_df[col_name].apply(
                lambda x: f"{x:+.1f}%" if pd.notna(x) else "‚Äî"
            )
        if "Market Cap ($B)" in display_df.columns:
            display_df["Market Cap ($B)"] = display_df["Market Cap ($B)"].apply(
                lambda x: f"${x:.1f}B" if pd.notna(x) else "‚Äî"
            )
        if "Revenue ($M)" in display_df.columns:
            display_df["Revenue ($M)"] = display_df["Revenue ($M)"].apply(
                lambda x: f"${x:,.0f}M" if pd.notna(x) else "‚Äî"
            )
        st.dataframe(display_df, use_container_width=True, hide_index=True)
    elif data_loaded:
        st.warning("No stock data could be fetched. Check network connection.")

# ====================  TAB 3: EARNINGS CALL INTELLIGENCE  ==================

TRANSCRIPT_DIR = DATA_DIR / "transcripts"

# Keywords to search for across transcripts ‚Äî grouped by theme
KEYWORD_GROUPS = {
    "Testing / QA": ["testing", "quality assurance", "QA ", "test automation", "code quality", "unit test", "regression test", "continuous testing"],
    "Validation": ["validation", "validate", "verification", "verify", "code review"],
    "Governance": ["governance", "guardrails", "compliance", "audit", "oversight", "responsible AI"],
    "Security": ["security testing", "vulnerability", "penetration test", "cybersecurity", "threat detection", "zero trust"],
    "AI Safety": ["AI safety", "model safety", "hallucination", "alignment", "red team"],
    "AI Agents": ["AI agent", "agentic", "copilot", "autonomous", "agent framework"],
    "Code Gen": ["code generation", "AI coding", "AI-generated code", "vibe coding", "AI developer"],
    "Observability": ["observability", "monitoring", "telemetry", "tracing", "logging"],
}

ALL_KEYWORDS = []
for group_kws in KEYWORD_GROUPS.values():
    ALL_KEYWORDS.extend(group_kws)


@st.cache_data(ttl=3600, show_spinner=False)
def load_all_transcripts():
    """Load all transcripts from disk and index keyword mentions."""
    if not TRANSCRIPT_DIR.exists():
        return [], pd.DataFrame()

    transcripts = []
    for ticker_dir in sorted(TRANSCRIPT_DIR.iterdir()):
        if not ticker_dir.is_dir():
            continue
        for fpath in sorted(ticker_dir.glob("*.json")):
            try:
                data = json.loads(fpath.read_text())
                transcripts.append(data)
            except Exception:
                continue

    if not transcripts:
        return [], pd.DataFrame()

    # Build keyword mention index
    rows = []
    for t in transcripts:
        ticker = t.get("ticker", "?")
        company = t.get("company", ticker)
        title = t.get("title", "")
        event_date = t.get("event_date", "")

        # Extract quarter from title (e.g. "Q3 2026 Earnings Call")
        quarter = title.split(" Earnings")[0].strip() if "Earnings" in title else title

        text_lower = t.get("text", "").lower()
        components = t.get("components", [])

        for group_name, keywords in KEYWORD_GROUPS.items():
            count = 0
            for kw in keywords:
                count += text_lower.count(kw.lower())
            rows.append({
                "Ticker": ticker,
                "Company": company,
                "Quarter": quarter,
                "Event Date": event_date,
                "Keyword Group": group_name,
                "Mentions": count,
            })

        # Also build per-keyword flat counts for the detailed view
        for kw in ALL_KEYWORDS:
            kw_count = text_lower.count(kw.lower())
            if kw_count > 0:
                rows.append({
                    "Ticker": ticker,
                    "Company": company,
                    "Quarter": quarter,
                    "Event Date": event_date,
                    "Keyword Group": "_individual",
                    "Keyword": kw,
                    "Mentions": kw_count,
                })

    return transcripts, pd.DataFrame(rows)


def find_keyword_context(text: str, keyword: str, window: int = 300) -> list[str]:
    """Find keyword occurrences in text with surrounding context."""
    lower_text = text.lower()
    lower_kw = keyword.lower()
    snippets = []
    start = 0
    while True:
        idx = lower_text.find(lower_kw, start)
        if idx == -1:
            break
        snippet_start = max(0, idx - window // 2)
        snippet_end = min(len(text), idx + len(keyword) + window // 2)
        snippet = text[snippet_start:snippet_end].strip()
        # Clean up
        snippet = snippet.replace("\r\n", " ").replace("\n", " ")
        snippets.append(f"...{snippet}...")
        start = idx + len(keyword)
        if len(snippets) >= 3:
            break
    return snippets


with tab3:
    st.header("Earnings Call Intelligence")

    with st.spinner("Loading transcripts..."):
        all_transcripts, keyword_df = load_all_transcripts()

    if not all_transcripts:
        st.warning("No transcripts found. Run the Koyfin transcript downloader first.")
        st.code("cd dashboards/software-shift && python3 koyfin_transcripts.py --help")
    else:
        # --- Map every transcript to a CALENDAR YEAR quarter using event_date ---
        def event_date_to_cy_quarter(event_date_str: str) -> Optional[str]:
            """Map event_date (YYYY-MM-DD) to Calendar Year quarter label."""
            if not event_date_str:
                return None
            try:
                dt = datetime.strptime(event_date_str[:10], "%Y-%m-%d")
            except (ValueError, TypeError):
                return None
            month = dt.month
            year = dt.year
            if month <= 3:
                return f"CY Q1 {year}"
            elif month <= 6:
                return f"CY Q2 {year}"
            elif month <= 9:
                return f"CY Q3 {year}"
            else:
                return f"CY Q4 {year}"

        # Assign calendar quarter to each transcript
        for t in all_transcripts:
            t["_cy_quarter"] = event_date_to_cy_quarter(t.get("event_date", ""))

        # Dynamically compute valid quarters from all transcripts that have data
        _all_cy_quarters_set = set()
        for t in all_transcripts:
            q = t.get("_cy_quarter")
            if q:
                _all_cy_quarters_set.add(q)

        def _quarter_sort_key(q_str):
            parts = q_str.split()  # "CY Q1 2025"
            return (int(parts[2]), int(parts[1][1]))

        ALL_CY_QUARTERS = sorted(_all_cy_quarters_set, key=_quarter_sort_key)

        # Earnings tab restricted to CY Q2-Q4 2025 per Steve's request
        VALID_CY_QUARTERS = ["CY Q2 2025", "CY Q3 2025", "CY Q4 2025"]

        valid_transcripts = [t for t in all_transcripts if t.get("_cy_quarter") in VALID_CY_QUARTERS]
        transcript_count = len(valid_transcripts)
        company_count = len(set(t["ticker"] for t in valid_transcripts))
        total_all = len(all_transcripts)

        st.markdown(f"**üìö {transcript_count} earnings call transcripts across {company_count} companies** (from {total_all} total Koyfin transcripts)")

        # Add CY Quarter to keyword_df using Event Date
        if not keyword_df.empty:
            keyword_df["CY Quarter"] = keyword_df["Event Date"].apply(event_date_to_cy_quarter)

        # Filter keyword_df to valid CY quarters & non-individual groups
        group_df = keyword_df[
            (keyword_df["Keyword Group"] != "_individual") &
            (keyword_df["CY Quarter"].isin(VALID_CY_QUARTERS))
        ].copy()

        # ------- TREND: Total mentions over CY quarters -------
        st.subheader("üìà Keyword Mention Trends Across Quarters")
        trend_data = group_df.groupby(["CY Quarter", "Keyword Group"])["Mentions"].sum().reset_index()
        if not trend_data.empty:
            # Ensure correct quarter ordering
            quarter_order = [q for q in VALID_CY_QUARTERS if q in trend_data["CY Quarter"].values]
            trend_data["CY Quarter"] = pd.Categorical(trend_data["CY Quarter"], categories=quarter_order, ordered=True)
            trend_data = trend_data.sort_values("CY Quarter")

            fig_trend = px.bar(
                trend_data, x="CY Quarter", y="Mentions", color="Keyword Group",
                barmode="stack",
                color_discrete_sequence=px.colors.qualitative.Set2,
                category_orders={"CY Quarter": quarter_order},
            )
            fig_trend.update_layout(
                template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                height=450, yaxis_title="Total Keyword Mentions", xaxis_title="",
                legend=dict(orientation="h", yanchor="top", y=-0.15, xanchor="center", x=0.5),
            )
            st.plotly_chart(fig_trend, use_container_width=True)

        # ------- NLP ENHANCED ANALYSIS -------
        nlp_cache = _load_earnings_nlp_cache()
        if nlp_cache and nlp_cache.get("thesis_scores"):
            st.markdown("---")
            st.subheader("üß† AI-Powered Sentiment Analysis")

            # Data freshness indicator
            _nlp_processed_at = nlp_cache.get("processed_at", "")
            _nlp_freshness_str = "unknown"
            _nlp_freshness_color = "#9E9E9E"
            if _nlp_processed_at:
                try:
                    _nlp_dt = datetime.fromisoformat(_nlp_processed_at)
                    _nlp_age_hours = (datetime.now() - _nlp_dt).total_seconds() / 3600
                    _nlp_freshness_str = _nlp_dt.strftime("%d %b %Y, %H:%M")
                    if _nlp_age_hours < 24:
                        _nlp_freshness_color = "#4CAF50"  # green ‚Äî fresh
                    elif _nlp_age_hours < 72:
                        _nlp_freshness_color = "#FF9800"  # orange ‚Äî ageing
                    else:
                        _nlp_freshness_color = "#f44336"  # red ‚Äî stale
                except Exception:
                    pass

            _total_snippets = sum(sum(len(r.get('snippets', [])) for r in results) for results in nlp_cache.get('sentiment_results', {}).values())
            _total_nlp_companies = len(nlp_cache.get('tickers_processed', []))

            st.markdown(
                f'<div style="display:flex; gap:16px; align-items:center; margin-bottom:8px; flex-wrap:wrap;">'
                f'<span style="font-size:0.82em; color:#aaa;">GPT-4o classified <b>{_total_snippets}</b> keyword snippets across <b>{_total_nlp_companies}</b> companies</span>'
                f'<span style="font-size:0.78em; background:#1a1a2e; padding:3px 8px; border-radius:10px;">'
                f'<span style="color:{_nlp_freshness_color};">‚óè</span> NLP cache: {_nlp_freshness_str}</span>'
                f'</div>',
                unsafe_allow_html=True,
            )

            # 1. Sentiment Breakdown ‚Äî per-quarter view (more intuitive than aggregate)
            nlp_col1, nlp_col2 = st.columns(2)
            with nlp_col1:
                # Build per-quarter sentiment data for the three specified quarters
                quarter_sentiment = []
                for ticker, ticker_results in nlp_cache.get("sentiment_results", {}).items():
                    for result in ticker_results:
                        event_date = result.get("event_date", "")
                        quarter = event_date_to_cy_quarter(event_date)
                        if quarter in VALID_CY_QUARTERS:
                            sentiment_counts = {"INCREASING": 0, "DECREASING": 0, "NEUTRAL": 0}
                            for snippet in result.get("snippets", []):
                                sent = snippet.get("sentiment", "NEUTRAL")
                                # Fold CUSTOMER_DEMAND into INCREASING (demand = positive signal)
                                if sent == "CUSTOMER_DEMAND":
                                    sent = "INCREASING"
                                if sent in sentiment_counts:
                                    sentiment_counts[sent] += 1
                            
                            for sent_type, count in sentiment_counts.items():
                                if count > 0:
                                    quarter_sentiment.append({
                                        "Quarter": quarter,
                                        "Sentiment": sent_type,
                                        "Count": count
                                    })

                if quarter_sentiment:
                    sent_df = pd.DataFrame(quarter_sentiment)
                    # Ensure correct quarter ordering
                    sent_df["Quarter"] = pd.Categorical(sent_df["Quarter"], categories=VALID_CY_QUARTERS, ordered=True)
                    sent_df = sent_df.sort_values("Quarter")
                    
                    fig_sent = px.bar(
                        sent_df, x="Quarter", y="Count", color="Sentiment",
                        color_discrete_map={"INCREASING": "#4CAF50", "DECREASING": "#f44336", "NEUTRAL": "#9E9E9E"},
                        barmode="stack"
                    )
                    _sent_q_range = f"{VALID_CY_QUARTERS[0]} ‚Äì {VALID_CY_QUARTERS[-1]}" if len(VALID_CY_QUARTERS) > 1 else VALID_CY_QUARTERS[0] if VALID_CY_QUARTERS else ""
                    fig_sent.update_layout(
                        template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                        height=350,
                        title=f"Sentiment by Quarter ({_sent_q_range})",
                        yaxis_title="Keyword Snippets", xaxis_title="",
                    )
                    st.plotly_chart(fig_sent, use_container_width=True)
                else:
                    st.info("No sentiment data available for Q2-Q4 2025")

            # 2. Management Guidance Signals
            with nlp_col2:
                guidance_totals = {"bullish": 0, "bearish": 0, "customer_signal": 0}
                for ticker_results in nlp_cache.get("guidance_results", {}).values():
                    for result in ticker_results:
                        for key in guidance_totals:
                            guidance_totals[key] += len(result.get(key, []))
                # Fold customer demand signals into bullish (positive signal)
                guidance_totals["bullish"] += guidance_totals.pop("customer_signal", 0)

                guidance_df = pd.DataFrame([
                    {"Signal": "Bullish (investing more)", "Count": guidance_totals["bullish"]},
                    {"Signal": "Bearish (cutting/reducing)", "Count": guidance_totals["bearish"]},
                ])
                fig_guid = px.bar(
                    guidance_df, x="Signal", y="Count",
                    color="Signal",
                    color_discrete_map={
                        "Bullish (investing more)": "#4CAF50",
                        "Bearish (cutting/reducing)": "#f44336",
                    },
                )
                fig_guid.update_layout(
                    template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                    height=350, showlegend=False,
                    title="Management Guidance Patterns",
                    yaxis_title="Pattern Matches", xaxis_title="",
                )
                st.plotly_chart(fig_guid, use_container_width=True)

                if guidance_totals["bullish"] > guidance_totals["bearish"] * 2:
                    st.success(f"üü¢ Management language strongly supports thesis ({guidance_totals['bullish']} bullish vs {guidance_totals['bearish']} bearish)")
                elif guidance_totals["bearish"] > guidance_totals["bullish"]:
                    st.warning(f"üü° More bearish than bullish management language detected")

            # 3. Q-over-Q Acceleration Chart
            qoq_data = nlp_cache.get("qoq_acceleration", {})
            quarterly_totals = qoq_data.get("quarterly_totals", [])
            if quarterly_totals and len(quarterly_totals) >= 2:
                st.markdown("---")
                st.subheader("üìà Q-over-Q Keyword Acceleration")
                accel = qoq_data.get("acceleration", {})
                trend_emoji = {"accelerating": "üöÄ", "decelerating": "üìâ", "stable": "‚û°Ô∏è"}.get(accel.get("overall_trend", ""), "‚ùì")
                st.caption(f"Trend: {trend_emoji} **{accel.get('overall_trend', 'unknown').upper()}** ¬∑ Latest growth: {accel.get('latest_growth_rate', 0):+.1f}% ¬∑ Acceleration: {accel.get('growth_rate_change', 0):+.1f}pp")

                # Build stacked area chart by keyword group ‚Äî show ALL quarters for trend visibility
                qoq_rows = []
                _qoq_quarters_seen = set()
                for qt in quarterly_totals:
                    _qoq_quarters_seen.add(qt["quarter"])
                    for group, count in qt.get("by_group", {}).items():
                        qoq_rows.append({"Quarter": qt["quarter"], "Group": group, "Mentions": count})

                if qoq_rows:
                    qoq_df = pd.DataFrame(qoq_rows)
                    # Sort quarters chronologically (e.g. "CY Q2 2025" ‚Üí (2025, 2))
                    def _qoq_sort_key(q_str):
                        parts = q_str.split()  # "CY Q2 2025" or "Q2 2025"
                        try:
                            if parts[0] == "CY":
                                return (int(parts[2]), int(parts[1][1]))
                            else:
                                return (int(parts[1]), int(parts[0][1]))
                        except (IndexError, ValueError):
                            return (0, 0)
                    quarter_order = sorted(_qoq_quarters_seen, key=_qoq_sort_key)
                    qoq_df["Quarter"] = pd.Categorical(qoq_df["Quarter"], categories=quarter_order, ordered=True)

                    fig_qoq = px.area(
                        qoq_df, x="Quarter", y="Mentions", color="Group",
                        color_discrete_sequence=px.colors.qualitative.Set2,
                        category_orders={"Quarter": quarter_order},
                    )
                    fig_qoq.update_layout(
                        template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                        height=400, yaxis_title="Total Keyword Mentions (All Companies)", xaxis_title="",
                        legend=dict(orientation="h", yanchor="top", y=-0.15, xanchor="center", x=0.5),
                    )
                    st.plotly_chart(fig_qoq, use_container_width=True)

            # 4. Company Thesis Rankings
            thesis_scores = nlp_cache.get("thesis_scores", {})
            if thesis_scores:
                st.markdown("---")
                st.subheader("üèÜ Company Thesis Ranking")
                st.caption("Composite score (0-10) based on sentiment analysis, management guidance, and keyword trends ‚Äî ranked strongest ‚Üí weakest")

                rankings = sorted(thesis_scores.values(), key=lambda x: x.get("thesis_score", 0), reverse=True)
                ranking_rows = []
                for _rank_idx, r in enumerate(rankings, 1):
                    direction_icon = {
                        "strongly_confirming": "üü¢üü¢",
                        "confirming": "üü¢",
                        "neutral": "‚ö™",
                        "challenging": "üî¥",
                        "strongly_challenging": "üî¥üî¥",
                    }.get(r.get("direction", ""), "‚ùì")
                    trend_icon = {
                        "accelerating": "üöÄ",
                        "growing": "üìà",
                        "stable": "‚û°Ô∏è",
                        "declining": "üìâ",
                    }.get(r.get("keyword_trend", ""), "‚ùì")

                    _sc = r.get("score_components", {})
                    ranking_rows.append({
                        "#": _rank_idx,
                        "Ticker": r["ticker"],
                        "Company": r.get("company", r["ticker"]),
                        "Score": r["thesis_score"],
                        "Direction": f"{direction_icon} {r.get('direction', '').replace('_', ' ').title()}",
                        "Trend": f"{trend_icon} {r.get('keyword_trend', '').replace('_', ' ').title()}",
                        "Qtrs": r.get("quarters_analyzed", 0),
                        "‚¨Ü Inc": r.get("sentiment_breakdown", {}).get("INCREASING", 0) + r.get("sentiment_breakdown", {}).get("CUSTOMER_DEMAND", 0),
                        "‚¨á Dec": r.get("sentiment_breakdown", {}).get("DECREASING", 0),
                        "Sent.": round(_sc.get("sentiment", 0), 1),
                        "Guid.": round(_sc.get("guidance", 0), 1),
                        "Trnd.": round(_sc.get("trend", 0), 1),
                        "Conf.": round(_sc.get("confidence_bonus", 0), 1),
                    })

                ranking_df = pd.DataFrame(ranking_rows)
                st.dataframe(
                    ranking_df,
                    use_container_width=True,
                    hide_index=True,
                    column_config={
                        "#": st.column_config.NumberColumn("Rank", width="small"),
                        "Score": st.column_config.ProgressColumn(
                            "Thesis Score",
                            min_value=0, max_value=10,
                            format="%.1f",
                        ),
                        "Sent.": st.column_config.NumberColumn("Sentiment (0-4)", help="Sentiment balance: INCREASING vs DECREASING", format="%.1f"),
                        "Guid.": st.column_config.NumberColumn("Guidance (0-3)", help="Bullish/bearish management guidance pattern matches", format="%.1f"),
                        "Trnd.": st.column_config.NumberColumn("Trend (0-2)", help="Q-over-Q keyword acceleration score", format="%.1f"),
                        "Conf.": st.column_config.NumberColumn("Conf. (0-1)", help="High-confidence signal bonus", format="%.1f"),
                    },
                )

                # Expandable per-company detail cards (top 5)
                with st.expander("üîç Top 5 Company Details ‚Äî Guidance Signals & Key Quotes"):
                    for r in rankings[:5]:
                        _t = r["ticker"]
                        _co = r.get("company", _t)
                        _dir_icon = {"strongly_confirming": "üü¢üü¢", "confirming": "üü¢", "neutral": "‚ö™", "challenging": "üî¥", "strongly_challenging": "üî¥üî¥"}.get(r.get("direction", ""), "‚ùì")
                        st.markdown(f"**{_dir_icon} {_t}** ‚Äî {_co} ¬∑ Score: **{r['thesis_score']}/10**")

                        _gc = r.get("guidance_counts", {})
                        _sb = r.get("sentiment_breakdown", {})
                        _detail_cols = st.columns(3)
                        _detail_cols[0].metric("üü¢ Bullish Guidance", _gc.get("bullish", 0) + _gc.get("customer_signal", 0))
                        _detail_cols[1].metric("üî¥ Bearish Guidance", _gc.get("bearish", 0))
                        _detail_cols[2].metric("üìä Total Snippets", _sb.get("INCREASING", 0) + _sb.get("DECREASING", 0) + _sb.get("CUSTOMER_DEMAND", 0) + _sb.get("NEUTRAL", 0))

                        # Show top quotes for this company
                        _kq = r.get("key_quotes", [])
                        if _kq:
                            for _q in _kq[:3]:
                                st.markdown(f'> *"{_q}"*')
                        st.markdown("---")

            # 5. Key Quotes Panel ‚Äî all available NLP-extracted quotes
            all_quotes = []
            _seen_quote_texts = set()
            for ticker, results in nlp_cache.get("sentiment_results", {}).items():
                for result in results:
                    event_date = result.get("event_date", "")
                    quarter = event_date_to_cy_quarter(event_date)
                    for snippet in result.get("snippets", []):
                        _q_text = snippet.get("key_quote", "")
                        if _q_text and snippet.get("confidence", 0) >= 3 and len(_q_text) > 15:
                            # Deduplicate by normalized text
                            _q_norm = _q_text.lower().strip()[:80]
                            if _q_norm not in _seen_quote_texts:
                                _seen_quote_texts.add(_q_norm)
                                all_quotes.append({
                                    "ticker": ticker,
                                    "event_date": event_date,
                                    "quarter": quarter or "?",
                                    "quote": _q_text,
                                    "sentiment": snippet.get("sentiment", "NEUTRAL"),
                                    "confidence": snippet.get("confidence", 1),
                                    "keyword": snippet.get("keyword", ""),
                                })

            if all_quotes:
                st.markdown("---")
                st.subheader("üíé Key Management Quotes")
                st.caption("Most impactful quotes from earnings calls (AI-extracted, confidence ‚â• 3/5)")

                # Sort by confidence, then sentiment value
                # Remap CUSTOMER_DEMAND ‚Üí INCREASING for display
                for q in all_quotes:
                    if q["sentiment"] == "CUSTOMER_DEMAND":
                        q["sentiment"] = "INCREASING"
                sentiment_order = {"INCREASING": 0, "DECREASING": 1, "NEUTRAL": 2}
                all_quotes.sort(key=lambda x: (-x["confidence"], sentiment_order.get(x["sentiment"], 3)))

                quote_cols = st.columns(2)
                for i, q in enumerate(all_quotes[:12]):
                    col = quote_cols[i % 2]
                    sent_icon = {"INCREASING": "üìà", "DECREASING": "üìâ", "NEUTRAL": "‚ö™"}.get(q["sentiment"], "‚ö™")
                    sent_color = {"INCREASING": "#4CAF50", "DECREASING": "#f44336", "NEUTRAL": "#9E9E9E"}.get(q["sentiment"], "#9E9E9E")
                    with col:
                        st.markdown(
                            f'<div style="border-left: 3px solid {sent_color}; padding: 8px 12px; margin: 6px 0; background: #1a1a2e; border-radius: 4px;">'
                            f'{sent_icon} <strong>{q["ticker"]}</strong> ¬∑ {q["event_date"][:10] if q["event_date"] else "?"} ¬∑ '
                            f'<span style="color: {sent_color};">{q["sentiment"]}</span> ¬∑ ‚≠ê{q["confidence"]}/5<br>'
                            f'<em>&ldquo;{q["quote"]}&rdquo;</em>'
                            f'</div>',
                            unsafe_allow_html=True,
                        )

            st.markdown("---")
        else:
            st.info("üí° **NLP analysis not yet available.** Run `python earnings_nlp.py` to generate AI-powered sentiment analysis across sample transcripts.")

        # ------- RELEVANT COMMENTS: Latest earnings calls on QA/testing -------
        st.subheader("üí¨ What Companies Are Saying About Testing & QA")
        st.caption("Relevant snippets from the most recent earnings call for each company ‚Äî ranked by total testing/governance keyword mentions")

        thesis_groups = ["Testing / QA", "Validation", "Governance", "AI Safety"]
        thesis_keywords = []
        for g in thesis_groups:
            thesis_keywords.extend(KEYWORD_GROUPS.get(g, []))

        if valid_transcripts:
            # Get most recent transcript per ticker (by event_date)
            latest_by_ticker: dict = {}
            for t in valid_transcripts:
                ticker = t.get("ticker", "?")
                edate = t.get("event_date", "")
                if ticker not in latest_by_ticker or edate > latest_by_ticker[ticker].get("event_date", ""):
                    latest_by_ticker[ticker] = t

            # Score each by thesis keyword mentions
            scored = []
            for ticker, t in latest_by_ticker.items():
                text_lower = t.get("text", "").lower()
                total_mentions = sum(text_lower.count(kw.lower()) for kw in thesis_keywords)
                if total_mentions > 0:
                    scored.append((total_mentions, ticker, t))
            scored.sort(reverse=True)

            if scored:
                # Show top 15 companies with relevant snippets
                for mention_count, ticker, t in scored[:15]:
                    company = t.get("company", ticker)
                    title = t.get("title", "")
                    edate = t.get("event_date", "")
                    text = t.get("text", "")

                    # Find best snippets across thesis keywords
                    all_snippets = []
                    for kw in thesis_keywords:
                        snips = find_keyword_context(text, kw, window=400)
                        for s in snips:
                            all_snippets.append((kw, s))
                    # Deduplicate overlapping snippets (keep first 4)
                    seen_starts = set()
                    unique_snippets = []
                    for kw, snip in all_snippets:
                        snip_key = snip[:80]
                        if snip_key not in seen_starts:
                            seen_starts.add(snip_key)
                            unique_snippets.append((kw, snip))
                        if len(unique_snippets) >= 4:
                            break

                    with st.expander(f"**{ticker}** ‚Äî {company} ¬∑ {edate} ¬∑ {mention_count} mentions"):
                        for kw, snip in unique_snippets:
                            # Highlight the keyword in the snippet
                            import re
                            highlighted = re.sub(
                                f"({re.escape(kw)})",
                                r"**\1**",
                                snip,
                                flags=re.IGNORECASE,
                            )
                            st.markdown(f"_{highlighted}_")
                            st.markdown("")
            else:
                st.info("No thesis-relevant mentions found in the latest transcripts.")

        # ------- KEYWORD SEARCH -------
        st.subheader("üîç Search Transcripts")
        search_kw = st.text_input("Search for any keyword across all transcripts", key="transcript_search")

        if search_kw:
            results = []
            for t in all_transcripts:
                text = t.get("text", "")
                count = text.lower().count(search_kw.lower())
                if count > 0:
                    snippets = find_keyword_context(text, search_kw)
                    results.append({
                        "ticker": t["ticker"],
                        "company": t.get("company", ""),
                        "title": t.get("title", ""),
                        "count": count,
                        "snippets": snippets,
                    })
            results.sort(key=lambda x: x["count"], reverse=True)

            st.markdown(f"**Found '{search_kw}' in {len(results)} transcripts** ({sum(r['count'] for r in results)} total mentions)")
            for r in results[:15]:
                with st.expander(f"**{r['ticker']}** ‚Äî {r['title']} ({r['count']} mentions)"):
                    for snip in r["snippets"]:
                        st.markdown(f"_{snip}_")

        # ------- NOTABLE QUOTES -------
        st.subheader("üí¨ Notable Quotes")
        quotes = load_json(EARNINGS_QUOTES_FILE)
        if quotes:
            for q in quotes:
                icon = signal_icon(q.get("signal", "neutral"))
                st.markdown(
                    f'<div class="quote-card">'
                    f'{icon} <strong>{q["speaker"]}</strong>, {q["company"]} ‚Äî {q["quarter"]}<br>'
                    f'<em>&ldquo;{q["quote"]}&rdquo;</em>'
                    f'</div>',
                    unsafe_allow_html=True,
                )
        else:
            # Pull NLP-extracted quotes when no manual quotes exist
            _auto_quotes_cache = _load_earnings_nlp_cache()
            if _auto_quotes_cache:
                _auto_quotes = []
                _auto_seen = set()
                for _aqt, _aqr_list in _auto_quotes_cache.get("sentiment_results", {}).items():
                    for _aqr in _aqr_list:
                        for _aqs in _aqr.get("snippets", []):
                            _aq_text = _aqs.get("key_quote", "")
                            if _aq_text and _aqs.get("confidence", 0) >= 4 and len(_aq_text) > 20:
                                _aq_norm = _aq_text.lower().strip()[:80]
                                if _aq_norm not in _auto_seen:
                                    _auto_seen.add(_aq_norm)
                                    _auto_quotes.append({
                                        "ticker": _aqt,
                                        "event_date": _aqr.get("event_date", ""),
                                        "quote": _aq_text,
                                        "sentiment": _aqs.get("sentiment", "NEUTRAL"),
                                        "confidence": _aqs.get("confidence", 1),
                                    })
                # Remap CUSTOMER_DEMAND ‚Üí INCREASING for display
                for _aq in _auto_quotes:
                    if _aq["sentiment"] == "CUSTOMER_DEMAND":
                        _aq["sentiment"] = "INCREASING"
                _auto_quotes.sort(key=lambda x: (-x["confidence"], x["sentiment"] != "INCREASING"))
                if _auto_quotes:
                    st.caption("Auto-extracted from earnings calls via NLP (confidence ‚â• 4/5). Use the form below to add curated quotes.")
                    for _aq in _auto_quotes[:8]:
                        _aq_icon = {"INCREASING": "üìà", "DECREASING": "üìâ", "NEUTRAL": "‚ö™"}.get(_aq["sentiment"], "‚ö™")
                        _aq_color = {"INCREASING": "#4CAF50", "DECREASING": "#f44336", "NEUTRAL": "#9E9E9E"}.get(_aq["sentiment"], "#9E9E9E")
                        st.markdown(
                            f'<div style="border-left: 3px solid {_aq_color}; padding: 8px 12px; margin: 6px 0; background: #1a1a2e; border-radius: 4px;">'
                            f'{_aq_icon} <strong>{_aq["ticker"]}</strong> ¬∑ {_aq["event_date"][:10] if _aq["event_date"] else "?"} ¬∑ '
                            f'<span style="color: {_aq_color};">{_aq["sentiment"]}</span> ¬∑ ‚≠ê{_aq["confidence"]}/5<br>'
                            f'<em>&ldquo;{_aq["quote"]}&rdquo;</em>'
                            f'</div>',
                            unsafe_allow_html=True,
                        )
                else:
                    st.info("No quotes yet. Use the form below to add real quotes from earnings calls.")
            else:
                st.info("No quotes yet. Use the form below to add real quotes from earnings calls.")

        st.markdown("---")
        st.subheader("‚ûï Add Quote")
        with st.form("add_quote_form", clear_on_submit=True):
            q_speaker = st.text_input("Speaker")
            q_company = st.text_input("Company")
            q_quarter = st.text_input("Quarter (e.g. Q1 2026)")
            q_quote = st.text_area("Quote")
            q_signal = st.selectbox("Signal", ["confirms", "neutral", "challenges"], key="quote_signal")
            q_submitted = st.form_submit_button("Add Quote")
            if q_submitted and q_quote:
                all_quotes = load_json(EARNINGS_QUOTES_FILE)
                all_quotes.append({
                    "speaker": q_speaker,
                    "company": q_company,
                    "quarter": q_quarter,
                    "quote": q_quote,
                    "signal": q_signal,
                    "added": datetime.now().isoformat(),
                })
                save_json(EARNINGS_QUOTES_FILE, all_quotes)
                st.success("Quote added!")
                st.rerun()

# ========================  TAB 4: FUNDING & M&A  ==========================
with tab4:
    st.header("Funding & M&A Tracker")
    st.caption("VC funding, M&A, and market reports in testing, QA, governance, and security")

    deals = load_json(FUNDING_DEALS_FILE)
    if deals:
        deals_df = pd.DataFrame(deals)
        deals_df["date"] = pd.to_datetime(deals_df["date"], errors="coerce")
        deals_df = deals_df.sort_values("date", ascending=False)

        # Summary metrics
        total_deals = len(deals_df)
        total_value = deals_df["amount_m"].sum()
        ma_count = len(deals_df[deals_df["type"] == "M&A"])
        funding_count = len(deals_df[deals_df["type"] == "Funding"])

        mc1, mc2, mc3, mc4 = st.columns(4)
        mc1.metric("Total Deals", total_deals)
        mc2.metric("M&A", ma_count)
        mc3.metric("Funding Rounds", funding_count)
        mc4.metric("Total Value", f"${total_value:,.0f}M")

        # Filter
        type_filter = st.selectbox(
            "Filter by type",
            ["All", "M&A", "Funding", "Secondary Sale"],
            key="deal_type_filter",
        )

        filtered_deals = deals_df
        if type_filter != "All":
            filtered_deals = deals_df[deals_df["type"] == type_filter]

        # Deal cards
        st.subheader("üìÖ Deal Timeline")
        for _, row in filtered_deals.iterrows():
            icon = signal_icon(row.get("signal", "neutral"))
            date_str = row["date"].strftime("%Y-%m-%d") if pd.notna(row["date"]) else "Unknown"
            amt = row.get("amount_m", 0)
            amount_str = f" ¬∑ **${amt:,.0f}M**" if amt and amt > 0 else ""
            deal_type = row.get("type", "")
            cat = row.get("category", "")
            desc = row.get("description", "")
            acquirer = row.get("acquirer", "")
            company = row.get("company", "")

            if deal_type == "M&A" and acquirer:
                headline = f"**{acquirer}** ‚Üí **{company}**"
            else:
                headline = f"**{company}**"

            # Build unverified badge and source link
            _unverified = row.get("unverified", False)
            _source_url = row.get("source_url", "")
            _verification_badge = ""
            if _unverified:
                _verification_badge = " ¬∑ ‚ö†Ô∏è _unverified_"
            _source_link = ""
            if _source_url:
                _source_link = f" ¬∑ [source]({_source_url})"

            _source_html = f' ¬∑ <a href="{_source_url}" style="color:#4FC3F7;">source</a>' if _source_url else ""
            st.markdown(
                f'<div style="background:#1A1D23; border:1px solid #2D3139; padding:12px 16px; margin:8px 0; border-radius:8px;">'
                f'{icon} <strong>{date_str}</strong> ¬∑ {deal_type} ¬∑ {cat}{amount_str}{_verification_badge}{_source_html}<br>'
                f'{headline}<br>'
                f'<em>{desc}</em>'
                f'</div>',
                unsafe_allow_html=True,
            )

        # Chart: deal value over time (bar chart)
        st.subheader("üìä Deal Values")
        funded = deals_df[deals_df["amount_m"] > 0].copy().sort_values("date")
        if not funded.empty:
            # Separate mega-deals (>$5B) from normal deals for chart readability
            mega_deals = funded[funded["amount_m"] >= 5000]
            chart_deals = funded[funded["amount_m"] < 5000]

            # Show callout for mega-deals excluded from chart
            if not mega_deals.empty:
                for _, mega in mega_deals.iterrows():
                    acq = mega.get("acquirer", "")
                    co = mega.get("company", "")
                    label = f"{acq}/{co}" if acq else co
                    st.info(f"üêã **{label}: ${mega['amount_m']:,.0f}M** ‚Äî excluded from chart for scale. {mega.get('description', '')}")

            if not chart_deals.empty:
                fig_deals = px.bar(
                    chart_deals, x="date", y="amount_m",
                    color="type",
                    hover_data=["company", "description"],
                    labels={"amount_m": "Deal Value ($M)", "date": ""},
                    color_discrete_map={"M&A": "#4FC3F7", "Funding": "#81C784", "Secondary Sale": "#FFB74D"},
                )
                fig_deals.update_layout(
                    template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                    height=400, yaxis_title="Deal Value ($M)", xaxis_title="",
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
                )
                st.plotly_chart(fig_deals, use_container_width=True)
            else:
                st.caption("All deals are mega-deals ‚Äî no chart to display at normal scale.")
    else:
        st.info("No deals tracked yet. Add one below.")

    st.markdown("---")
    st.subheader("‚ûï Add Deal")
    with st.form("add_deal_form", clear_on_submit=True):
        dc1, dc2 = st.columns(2)
        with dc1:
            d_date = st.date_input("Date", value=date.today(), key="deal_date")
            d_type = st.selectbox("Type", ["M&A", "Funding", "Market Report", "Other"], key="deal_type")
            d_company = st.text_input("Company / Target")
            d_acquirer = st.text_input("Acquirer (if M&A)")
        with dc2:
            d_amount = st.number_input("Amount ($M)", min_value=0, value=0, key="deal_amount")
            d_category = st.text_input("Category (e.g. Code Review, Cloud Security)")
            d_signal = st.selectbox("Signal", ["confirms", "neutral", "challenges"], key="deal_signal")
        d_desc = st.text_area("Description")
        d_submitted = st.form_submit_button("Add Deal")
        if d_submitted and d_company:
            all_deals = load_json(FUNDING_DEALS_FILE)
            all_deals.append({
                "date": d_date.isoformat(),
                "type": d_type,
                "company": d_company,
                "acquirer": d_acquirer,
                "amount_m": d_amount,
                "category": d_category,
                "description": d_desc,
                "signal": d_signal,
                "added": datetime.now().isoformat(),
            })
            save_json(FUNDING_DEALS_FILE, all_deals)
            st.success("Deal added!")
            st.rerun()

# ========================  TAB 5: SEARCH TRENDS  ==========================
with tab5:
    st.header("Search Trends")
    st.caption("Google Trends data comparing testing/QA interest vs dev tools interest over the last 12 months")

    with st.spinner("Fetching Google Trends data..."):
        trends_data = fetch_google_trends()

    if "_error" in trends_data:
        st.error(f"Could not fetch trends: {trends_data['_error']}")
        st.info("This often happens due to pytrends rate limiting. Data is cached for 4 hours once fetched successfully.")
    else:
        # Build dataframe
        trend_rows = []
        for term, series in trends_data.items():
            if isinstance(series, str) and series.startswith("error"):
                continue
            if series is None:
                continue
            if term in TRENDS_GROUP_A:
                group = "Testing / QA"
            elif term in TRENDS_GROUP_C:
                group = "AI Coding Tools"
            else:
                group = "Dev Tools"
            try:
                for dt, val in series.items():
                    trend_rows.append({"Date": dt, "Term": term, "Group": group, "Interest": val})
            except Exception:
                continue

        if trend_rows:
            trends_df = pd.DataFrame(trend_rows)

            # Individual terms
            st.subheader("üìà Individual Search Terms")
            fig_terms = px.line(
                trends_df, x="Date", y="Interest", color="Term",
                line_dash="Group",
            )
            fig_terms.update_layout(
                template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                height=500, yaxis_title="Search Interest (relative)", xaxis_title="",
                legend=dict(orientation="h", yanchor="top", y=-0.15, xanchor="center", x=0.5),
            )
            st.plotly_chart(fig_terms, use_container_width=True)

            # Group averages
            st.subheader("üìä Group Average Comparison")
            group_avg = trends_df.groupby(["Date", "Group"])["Interest"].mean().reset_index()
            fig_group = px.line(
                group_avg, x="Date", y="Interest", color="Group",
                color_discrete_map={"Testing / QA": "#4FC3F7", "Dev Tools": "#FF8A65", "AI Coding Tools": "#AB47BC"},
            )
            fig_group.update_traces(line_width=3)
            fig_group.update_layout(
                template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                height=400, yaxis_title="Avg Search Interest", xaxis_title="",
                hovermode="x unified",
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
            )
            st.plotly_chart(fig_group, use_container_width=True)

            # Summary metric
            latest_month = group_avg.groupby("Group").tail(4).groupby("Group")["Interest"].mean()
            earliest_month = group_avg.groupby("Group").head(4).groupby("Group")["Interest"].mean()

            c1, c2, c3 = st.columns(3)
            _trend_cols = [c1, c2, c3]
            for i, group in enumerate(["Testing / QA", "Dev Tools", "AI Coding Tools"]):
                col = _trend_cols[i]
                latest_val = latest_month.get(group, 0)
                earliest_val = earliest_month.get(group, 1)
                change = ((latest_val / earliest_val) - 1) * 100 if earliest_val > 0 else 0
                col.metric(
                    f"{group} ‚Äî 12mo Trend",
                    f"{latest_val:.0f}",
                    delta=f"{change:+.1f}% vs 12mo ago",
                )
        else:
            st.warning("No trend data available. This may be due to pytrends rate limiting.")
            st.info("üí° Tip: Try refreshing in a few minutes. Data is cached for 4 hours once fetched.")

# ========================  TAB 6: INCIDENT TRACKER  ========================
with tab6:
    st.header("üö® Incident Tracker")
    st.caption("Production incidents & security failures ‚Äî Leapwork's strongest sales catalyst: 'AI-generated code is breaking things, you need testing'")

    # --- Load Category C articles ---
    _all_articles_raw = load_news_store()
    _all_articles_json = load_json(NEWS_STORE)  # raw JSON for snippet access
    _scores_raw = {}
    if SCORES_FILE.exists():
        try:
            _scores_raw = json.loads(SCORES_FILE.read_text())
        except Exception:
            pass

    # Build unified list from raw JSON (preserving string dates)
    _cat_c_articles = []
    for a in _all_articles_json:
        if a.get("source_category") != "C":
            continue
        uid = a.get("uid", "")
        pub_date = a.get("published", "")
        title = a.get("title", "")
        source = a.get("source", "unknown")
        snippet = a.get("snippet", "")
        url = a.get("url", a.get("link", ""))

        # Get scores
        sc = _scores_raw.get(uid, {})
        gr_score = 0
        if sc.get("schema_version") == 2:
            gr_score = sc.get("dimension_scores", {}).get("GR", 0)
        elif sc.get("relevance"):
            gr_score = sc.get("relevance", 0)

        # AI-attribution detection
        _ai_keywords = [
            "ai failure", "automation outage", "pipeline error", "ai-generated",
            "release rollback", "deployment failure", "ai system", "ai agent",
            "automated deploy", "ci/cd", "continuous integration", "machine learning",
            "model failure", "llm", "chatbot", "copilot", "code generation",
            "automated pipeline", "devops failure", "ai vulnerability",
            "ai-powered", "generative ai", "autonomous", "auto-generated",
        ]
        _text_lower = (title + " " + snippet).lower()
        ai_related = any(kw in _text_lower for kw in _ai_keywords)

        _cat_c_articles.append({
            "uid": uid,
            "date": pub_date,
            "title": title,
            "source": source,
            "snippet": snippet,
            "url": url,
            "gr_score": gr_score,
            "ai_related": ai_related,
        })

    # Parse dates
    for a in _cat_c_articles:
        try:
            a["_dt"] = datetime.strptime(a["date"][:10], "%Y-%m-%d") if a["date"] else None
        except Exception:
            a["_dt"] = None

    # Filter to sidebar date range
    _cat_c_filtered = [
        a for a in _cat_c_articles
        if a["_dt"] and a["_dt"].date() >= date_start and a["_dt"].date() <= date_end
    ]

    _total_incidents = len(_cat_c_filtered)
    _ai_incidents = sum(1 for a in _cat_c_filtered if a["ai_related"])
    _ai_pct = (_ai_incidents / _total_incidents * 100) if _total_incidents > 0 else 0
    _avg_gr = (sum(a["gr_score"] for a in _cat_c_filtered) / _total_incidents) if _total_incidents > 0 else 0

    # Summary metrics
    ic1, ic2, ic3, ic4 = st.columns(4)
    ic1.metric("Total Incidents", _total_incidents)
    ic2.metric("AI/Automation-Related", _ai_incidents, delta=f"{_ai_pct:.0f}%")
    ic3.metric("Avg GR Score", f"{_avg_gr:.1f}")
    ic4.metric("Sources", len(set(a["source"] for a in _cat_c_filtered)))

    if _total_incidents == 0:
        st.warning("No Category C (Security/Failures) articles found in the selected date range. Try expanding the date range or running the scanner.")
    else:
        # === 1. Incident Timeline (weekly buckets) ===
        st.subheader("üìÖ Incident Timeline")
        st.caption("Category C articles over time ‚Äî color-coded by governance relevance (GR score)")

        _timeline_df = pd.DataFrame([
            {
                "date": a["_dt"],
                "gr_score": a["gr_score"],
                "ai_related": a["ai_related"],
                "title": a["title"],
                "source": a["source"],
            }
            for a in _cat_c_filtered if a["_dt"]
        ])

        if not _timeline_df.empty:
            _timeline_df["week"] = _timeline_df["date"].dt.to_period("W").apply(lambda p: p.start_time)
            _weekly_counts = _timeline_df.groupby("week").agg(
                count=("date", "size"),
                avg_gr=("gr_score", "mean"),
                ai_count=("ai_related", "sum"),
            ).reset_index()

            fig_timeline = go.Figure()

            # Bar chart of weekly incident counts
            fig_timeline.add_trace(go.Bar(
                x=_weekly_counts["week"],
                y=_weekly_counts["count"],
                name="Incidents",
                marker_color=[
                    f"rgba(255, {max(0, 200 - int(gr * 40))}, {max(0, 100 - int(gr * 20))}, 0.7)"
                    for gr in _weekly_counts["avg_gr"]
                ],
                text=_weekly_counts["count"],
                textposition="outside",
                hovertemplate="Week: %{x}<br>Incidents: %{y}<br>Avg GR: %{customdata:.1f}<extra></extra>",
                customdata=_weekly_counts["avg_gr"],
            ))

            # Trend line using numpy polyfit
            if len(_weekly_counts) >= 3:
                _x_num = np.arange(len(_weekly_counts))
                _coeffs = np.polyfit(_x_num, _weekly_counts["count"].values, 1)
                _trend_y = np.polyval(_coeffs, _x_num)
                fig_timeline.add_trace(go.Scatter(
                    x=_weekly_counts["week"],
                    y=_trend_y,
                    mode="lines",
                    name=f"Trend ({'‚Üë increasing' if _coeffs[0] > 0 else '‚Üì decreasing'})",
                    line=dict(color="#FFD54F", width=2, dash="dash"),
                ))

            fig_timeline.update_layout(
                template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                height=400, yaxis_title="Incident Count", xaxis_title="",
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
                margin=dict(t=40),
            )
            st.plotly_chart(fig_timeline, use_container_width=True)

        # === 2. Incident Severity Heatmap ===
        st.subheader("üî• Incident Intensity Heatmap")
        st.caption("Monthly incident density ‚Äî brighter = more incidents")

        _heatmap_df = pd.DataFrame([
            {"date": a["_dt"], "title": a["title"]}
            for a in _cat_c_filtered if a["_dt"]
        ])

        if not _heatmap_df.empty:
            _heatmap_df["year"] = _heatmap_df["date"].dt.year
            _heatmap_df["month"] = _heatmap_df["date"].dt.month
            _monthly = _heatmap_df.groupby(["year", "month"]).size().reset_index(name="count")

            # Pivot for heatmap
            _years = sorted(_monthly["year"].unique())
            _month_names = ["Jan", "Feb", "Mar", "Apr", "May", "Jun", "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"]

            _pivot = _monthly.pivot(index="year", columns="month", values="count").fillna(0)
            _pivot.columns = [_month_names[int(m) - 1] for m in _pivot.columns]

            fig_heatmap = go.Figure(data=go.Heatmap(
                z=_pivot.values,
                x=_pivot.columns,
                y=[str(y) for y in _pivot.index],
                colorscale="YlOrRd",
                text=_pivot.values.astype(int),
                texttemplate="%{text}",
                hovertemplate="Year: %{y}<br>Month: %{x}<br>Incidents: %{z}<extra></extra>",
            ))
            fig_heatmap.update_layout(
                template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                height=max(200, len(_years) * 60 + 100),
                yaxis_title="", xaxis_title="",
                margin=dict(t=20),
            )
            st.plotly_chart(fig_heatmap, use_container_width=True)

            # Highlight spike months
            _avg_count = _monthly["count"].mean()
            _spikes = _monthly[_monthly["count"] > _avg_count * 1.5]
            if not _spikes.empty:
                _spike_strs = [f"{_month_names[int(r['month'])-1]} {int(r['year'])} ({int(r['count'])} incidents)" for _, r in _spikes.iterrows()]
                st.info(f"üìà **Spike months** (>1.5√ó average of {_avg_count:.0f}): {', '.join(_spike_strs)}")

        # === 3. AI-Attribution Analysis ===
        st.subheader("ü§ñ AI-Attribution Analysis")
        st.caption("What % of incidents mention AI/automation? Should be increasing as AI-generated code proliferates.")

        _ai_df = pd.DataFrame([
            {"date": a["_dt"], "ai_related": a["ai_related"]}
            for a in _cat_c_filtered if a["_dt"]
        ])

        if not _ai_df.empty:
            _ai_df["month"] = _ai_df["date"].dt.to_period("M").apply(lambda p: p.start_time)
            _ai_monthly = _ai_df.groupby("month").agg(
                total=("ai_related", "size"),
                ai_count=("ai_related", "sum"),
            ).reset_index()
            _ai_monthly["ai_pct"] = (_ai_monthly["ai_count"] / _ai_monthly["total"] * 100).round(1)

            fig_ai = go.Figure()

            # Stacked bar: AI-related vs non-AI
            fig_ai.add_trace(go.Bar(
                x=_ai_monthly["month"],
                y=_ai_monthly["ai_count"],
                name="AI/Automation-Related",
                marker_color="#FF6B6B",
            ))
            fig_ai.add_trace(go.Bar(
                x=_ai_monthly["month"],
                y=_ai_monthly["total"] - _ai_monthly["ai_count"],
                name="Other Incidents",
                marker_color="#4A4A6A",
            ))

            # AI % line
            fig_ai.add_trace(go.Scatter(
                x=_ai_monthly["month"],
                y=_ai_monthly["ai_pct"],
                mode="lines+markers",
                name="AI Attribution %",
                line=dict(color="#FFD54F", width=3),
                marker=dict(size=8),
                yaxis="y2",
            ))

            fig_ai.update_layout(
                template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                height=420, barmode="stack",
                yaxis=dict(title="Incident Count"),
                yaxis2=dict(title="AI Attribution %", side="right", overlaying="y", showgrid=False, range=[0, 100]),
                xaxis=dict(title=""),
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
                margin=dict(t=40),
            )
            st.plotly_chart(fig_ai, use_container_width=True)

            # Show trend in AI attribution
            if len(_ai_monthly) >= 3:
                _first_half = _ai_monthly.head(len(_ai_monthly) // 2)["ai_pct"].mean()
                _second_half = _ai_monthly.tail(len(_ai_monthly) // 2)["ai_pct"].mean()
                _delta = _second_half - _first_half
                if _delta > 2:
                    st.success(f"üìà AI-attribution rate is **increasing**: {_first_half:.1f}% ‚Üí {_second_half:.1f}% (+{_delta:.1f}pp)")
                elif _delta < -2:
                    st.warning(f"üìâ AI-attribution rate is **decreasing**: {_first_half:.1f}% ‚Üí {_second_half:.1f}% ({_delta:.1f}pp)")
                else:
                    st.info(f"‚û°Ô∏è AI-attribution rate is **stable**: ~{_ai_monthly['ai_pct'].mean():.1f}%")

        # === 4. Pain Index ===
        st.subheader("üí¢ Pain Index")
        st.caption("Composite metric: Incident Count √ó Avg Severity (GR Score) √ó AI-Attribution Rate ‚Äî the single most compelling chart for Leapwork's narrative")

        _pain_df = pd.DataFrame([
            {"date": a["_dt"], "gr_score": a["gr_score"], "ai_related": a["ai_related"]}
            for a in _cat_c_filtered if a["_dt"]
        ])

        if not _pain_df.empty:
            _pain_df["month"] = _pain_df["date"].dt.to_period("M").apply(lambda p: p.start_time)
            _pain_monthly = _pain_df.groupby("month").agg(
                count=("date", "size"),
                avg_gr=("gr_score", "mean"),
                ai_rate=("ai_related", "mean"),
            ).reset_index()

            # Pain Index = count √ó (1 + avg_gr/10) √ó (1 + ai_rate)
            # Normalized so it scales reasonably
            _pain_monthly["pain_index"] = (
                _pain_monthly["count"]
                * (1 + _pain_monthly["avg_gr"] / 10)
                * (1 + _pain_monthly["ai_rate"])
            ).round(1)

            fig_pain = go.Figure()

            # Area chart for pain index
            fig_pain.add_trace(go.Scatter(
                x=_pain_monthly["month"],
                y=_pain_monthly["pain_index"],
                fill="tozeroy",
                name="Pain Index",
                line=dict(color="#FF4444", width=3),
                fillcolor="rgba(255, 68, 68, 0.2)",
            ))

            # Add incident count on secondary axis for context
            fig_pain.add_trace(go.Bar(
                x=_pain_monthly["month"],
                y=_pain_monthly["count"],
                name="Raw Incident Count",
                marker_color="rgba(100, 100, 200, 0.3)",
                yaxis="y2",
            ))

            fig_pain.update_layout(
                template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                height=420,
                yaxis=dict(title="Pain Index", side="left"),
                yaxis2=dict(title="Incident Count", side="right", overlaying="y", showgrid=False),
                xaxis=dict(title=""),
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
                margin=dict(t=40),
            )
            st.plotly_chart(fig_pain, use_container_width=True)

            # Pain trend
            if len(_pain_monthly) >= 2:
                _recent_pain = _pain_monthly.tail(3)["pain_index"].mean()
                _earlier_pain = _pain_monthly.head(3)["pain_index"].mean()
                if _earlier_pain > 0:
                    _pain_change = ((_recent_pain / _earlier_pain) - 1) * 100
                    _pc1, _pc2, _pc3 = st.columns(3)
                    _pc1.metric("Current Pain Index", f"{_recent_pain:.1f}")
                    _pc2.metric("Pain Trend", f"{_pain_change:+.0f}%", delta_color="inverse")
                    _pc3.metric("Peak Pain", f"{_pain_monthly['pain_index'].max():.1f}")

        # === 5. Recent Incidents Table ===
        st.markdown("---")
        st.subheader("üìã Recent Incidents")
        st.caption("Category C articles ‚Äî security failures, production incidents, outages")

        _table_data = sorted(_cat_c_filtered, key=lambda x: x["date"] or "", reverse=True)

        # Load translations for non-English titles in incident table
        _incident_translations = _load_title_translations()

        if _table_data:
            _table_rows = []
            for a in _table_data[:100]:
                _raw_title = a["title"][:80]
                _en = _incident_translations.get(a.get("uid", ""))
                _display_title = f"{_raw_title}\nüåê {_en}" if _en else _raw_title
                _table_rows.append({
                    "Date": a["date"][:10] if a["date"] else "?",
                    "Title": _display_title,
                    "Source": a["source"],
                    "GR Score": a["gr_score"],
                    "AI-Related": "ü§ñ Yes" if a["ai_related"] else "No",
                    "Snippet": (a["snippet"] or "")[:120],
                })

            _table_df = pd.DataFrame(_table_rows)
            st.dataframe(
                _table_df,
                use_container_width=True,
                hide_index=True,
                column_config={
                    "GR Score": st.column_config.ProgressColumn(
                        "GR Score", min_value=0, max_value=10, format="%d",
                    ),
                    "Title": st.column_config.TextColumn("Title", width="large"),
                    "Snippet": st.column_config.TextColumn("Snippet", width="large"),
                },
            )
            st.caption(f"Showing top 100 of {len(_table_data)} incidents. Most articles are unscored (GR=0) ‚Äî run the v2 scorer to populate scores.")
        else:
            st.info("No incidents to display.")

# ========================  TAB 7: LEADING INDICATORS  ======================
with tab7:
    st.header("üîÆ Leading Indicators")
    st.caption("Forward-looking signals: search trends, job market, funding pipeline, and market sizing")

    # ---- Section 1: Enhanced Google Trends ----
    st.subheader("üìà Google Trends ‚Äî Testing vs Building")
    st.caption("Comparing search interest: are people searching more for AI testing or AI coding?")

    # Reuse the existing trends data from Tab 5
    # (already fetched and cached)
    _trends_available = False
    try:
        _trends_data_li = fetch_google_trends()
        if "_error" not in _trends_data_li:
            _trends_available = True
    except Exception:
        pass

    if _trends_available:
        _trend_rows_li = []
        for term, series in _trends_data_li.items():
            if isinstance(series, str) and series.startswith("error"):
                continue
            if series is None:
                continue
            if term in TRENDS_GROUP_A:
                group = "Testing / QA"
            elif term in TRENDS_GROUP_C:
                group = "AI Coding Tools"
            else:
                group = "Dev Tools"
            try:
                for dt, val in series.items():
                    _trend_rows_li.append({"Date": dt, "Term": term, "Group": group, "Interest": val})
            except Exception:
                continue

        if _trend_rows_li:
            _trends_df_li = pd.DataFrame(_trend_rows_li)

            # Group comparison chart
            _group_avg_li = _trends_df_li.groupby(["Date", "Group"])["Interest"].mean().reset_index()
            fig_trends_li = px.line(
                _group_avg_li, x="Date", y="Interest", color="Group",
                color_discrete_map={"Testing / QA": "#4FC3F7", "Dev Tools": "#FF8A65", "AI Coding Tools": "#AB47BC"},
            )
            fig_trends_li.update_traces(line_width=3)
            fig_trends_li.update_layout(
                template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                height=350, yaxis_title="Avg Search Interest", xaxis_title="",
                hovermode="x unified",
                legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
            )
            st.plotly_chart(fig_trends_li, use_container_width=True)

            # Ratio chart: Testing / Dev Tools
            _ratio_df = _group_avg_li.pivot(index="Date", columns="Group", values="Interest").dropna()
            if "Testing / QA" in _ratio_df.columns and "Dev Tools" in _ratio_df.columns:
                _ratio_df["Ratio"] = _ratio_df["Testing / QA"] / _ratio_df["Dev Tools"].replace(0, np.nan)
                _ratio_df = _ratio_df.dropna(subset=["Ratio"])
                if not _ratio_df.empty:
                    fig_ratio = go.Figure()
                    fig_ratio.add_trace(go.Scatter(
                        x=_ratio_df.index,
                        y=_ratio_df["Ratio"],
                        fill="tozeroy",
                        name="Testing / Dev Tools Ratio",
                        line=dict(color="#4FC3F7", width=2),
                        fillcolor="rgba(79, 195, 247, 0.15)",
                    ))
                    fig_ratio.add_hline(y=1.0, line_dash="dash", line_color="grey", annotation_text="Parity")
                    fig_ratio.update_layout(
                        template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                        height=250, yaxis_title="Interest Ratio", xaxis_title="",
                        margin=dict(t=20),
                    )
                    st.plotly_chart(fig_ratio, use_container_width=True)
                    _latest_ratio = _ratio_df["Ratio"].iloc[-1]
                    if _latest_ratio > 1:
                        st.success(f"üìà Testing search interest **exceeds** dev tools interest (ratio: {_latest_ratio:.2f})")
                    else:
                        st.info(f"Dev tools search interest still leads (ratio: {_latest_ratio:.2f})")
        else:
            st.info("No trend data available. Check the Search Trends tab for more details.")
    else:
        st.warning("Google Trends data unavailable. Check Search Trends tab for details.")
        st.info("üí° Terms tracked: 'Agentic AI testing', 'Agentic software development' vs 'Agentic QA', 'AI agent testing', 'AI agent validation' + 'Claude Code', 'ChatGPT Codex'")

    st.markdown("---")

    # Job Market Signal section removed per Steve's feedback

    # ---- Section 3: Funding Pipeline ----
    st.subheader("üí∞ Funding Pipeline ‚Äî Testing/QA/Governance")
    st.caption("VC funding flowing into the testing & validation space ‚Äî timeline and running total")

    _funding_deals = load_json(FUNDING_DEALS_FILE)

    if _funding_deals:
        _fund_df = pd.DataFrame(_funding_deals)
        _fund_df["date"] = pd.to_datetime(_fund_df["date"], errors="coerce")
        _fund_df = _fund_df.dropna(subset=["date"]).sort_values("date")
        _fund_df["amount_m"] = pd.to_numeric(_fund_df["amount_m"], errors="coerce").fillna(0)

        # Exclude mega-deals for chart scale (e.g. Google/Wiz $32B)
        _fund_chart = _fund_df[_fund_df["amount_m"] < 5000].copy()
        if "unverified" not in _fund_chart.columns:
            _fund_chart["unverified"] = False
        _fund_chart["unverified"] = _fund_chart["unverified"].fillna(False).astype(bool)
        _fund_chart["verified_label"] = _fund_chart["unverified"].apply(
            lambda x: "‚ö†Ô∏è Unverified" if x else "‚úÖ Verified"
        )

        _fc1, _fc2 = st.columns(2)

        with _fc1:
            # Timeline chart
            if not _fund_chart.empty:
                fig_fund = px.bar(
                    _fund_chart, x="date", y="amount_m",
                    color="verified_label",
                    hover_data=["company", "category", "description"],
                    labels={"amount_m": "Deal Value ($M)", "date": ""},
                    color_discrete_map={"‚úÖ Verified": "#81C784", "‚ö†Ô∏è Unverified": "#FFB74D"},
                )
                fig_fund.update_layout(
                    template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                    height=350, yaxis_title="Deal Value ($M)", xaxis_title="",
                    legend=dict(orientation="h", yanchor="bottom", y=1.02, xanchor="right", x=1),
                )
                st.plotly_chart(fig_fund, use_container_width=True)

        with _fc2:
            # Running total (cumulative)
            if not _fund_chart.empty:
                _fund_chart_sorted = _fund_chart.sort_values("date")
                _fund_chart_sorted["cumulative_m"] = _fund_chart_sorted["amount_m"].cumsum()

                fig_cum = go.Figure()
                fig_cum.add_trace(go.Scatter(
                    x=_fund_chart_sorted["date"],
                    y=_fund_chart_sorted["cumulative_m"],
                    fill="tozeroy",
                    name="Cumulative Investment ($M)",
                    line=dict(color="#81C784", width=3),
                    fillcolor="rgba(129, 199, 132, 0.2)",
                    text=_fund_chart_sorted["company"],
                    hovertemplate="Date: %{x}<br>Cumulative: $%{y:,.0f}M<br>Company: %{text}<extra></extra>",
                ))
                fig_cum.update_layout(
                    template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                    height=350, yaxis_title="Cumulative ($M)", xaxis_title="",
                )
                st.plotly_chart(fig_cum, use_container_width=True)

        # Summary metrics
        _total_funding = _fund_df[_fund_df["amount_m"] < 5000]["amount_m"].sum()
        _is_unverified = _fund_df["unverified"].fillna(False).astype(bool) if "unverified" in _fund_df.columns else pd.Series([False] * len(_fund_df))
        _verified_count = int((~_is_unverified).sum())
        _unverified_count = int(_is_unverified.sum())
        _fm1, _fm2, _fm3 = st.columns(3)
        _fm1.metric("Total Deals (excl. mega)", len(_fund_chart))
        _fm2.metric("Total Invested", f"${_total_funding:,.0f}M")
        _fm3.metric("Verification", f"‚úÖ{_verified_count} / ‚ö†Ô∏è{_unverified_count}")
    else:
        st.info("No funding data found. Check `data/funding_deals.json`.")

    st.markdown("---")

    # ---- Section 4: Market Sizing Estimates ----
    st.subheader("üìä Market Sizing Estimates")
    st.caption("Key market size estimates from analyst reports ‚Äî how big is the opportunity?")

    MARKET_SIZING_FILE = DATA_DIR / "market_sizing.json"
    _market_data = load_json(MARKET_SIZING_FILE)

    if _market_data:
        _mkt_df = pd.DataFrame(_market_data)

        # Current vs Forecast grouping
        _mkt_current = _mkt_df[~_mkt_df["metric"].str.contains("Forecast", case=False)].copy()
        _mkt_forecast = _mkt_df[_mkt_df["metric"].str.contains("Forecast", case=False)].copy()

        # Current market sizes bar chart
        if not _mkt_current.empty:
            _mkt_current = _mkt_current.sort_values("value_b", ascending=True)
            fig_mkt = px.bar(
                _mkt_current, x="value_b", y="metric",
                orientation="h",
                text="value_b",
                labels={"value_b": "Market Size ($B)", "metric": ""},
                color_discrete_sequence=["#4FC3F7"],
            )
            fig_mkt.update_traces(texttemplate="$%{text:.1f}B", textposition="outside")
            fig_mkt.update_layout(
                template="plotly_dark", paper_bgcolor="#0E1117", plot_bgcolor="#0E1117",
                height=max(250, len(_mkt_current) * 50 + 50),
                xaxis_title="Market Size ($B)", yaxis_title="",
                margin=dict(l=200),
                title="Current Market Sizes (2024)",
            )
            st.plotly_chart(fig_mkt, use_container_width=True)

        # Current vs Forecast comparison
        if not _mkt_current.empty and not _mkt_forecast.empty:
            # Pair current and forecast
            _pairs = []
            for _, cur in _mkt_current.iterrows():
                _base_name = cur["metric"]
                _forecast_row = _mkt_forecast[_mkt_forecast["metric"].str.startswith(_base_name)]
                if not _forecast_row.empty:
                    _fr = _forecast_row.iloc[0]
                    _cagr = ((_fr["value_b"] / cur["value_b"]) ** (1 / max(1, _fr["year"] - cur["year"])) - 1) * 100
                    _pairs.append({
                        "Market": _base_name,
                        f"Current ({int(cur['year'])})" : cur["value_b"],
                        f"Forecast ({int(_fr['year'])})": _fr["value_b"],
                        "CAGR": f"{_cagr:.1f}%",
                        "Source": cur["source"],
                    })

            if _pairs:
                st.subheader("üìà Growth Projections")
                _pairs_df = pd.DataFrame(_pairs)
                st.dataframe(_pairs_df, use_container_width=True, hide_index=True)

                # Total addressable market
                _tam_current = sum(p[list(p.keys())[1]] for p in _pairs)
                _tam_forecast = sum(p[list(p.keys())[2]] for p in _pairs)
                _tc1, _tc2, _tc3 = st.columns(3)
                _tc1.metric("Combined TAM (Current)", f"${_tam_current:.1f}B")
                _tc2.metric("Combined TAM (Forecast)", f"${_tam_forecast:.1f}B")
                _tc3.metric("Growth Multiple", f"{_tam_forecast/_tam_current:.1f}√ó" if _tam_current > 0 else "‚Äî")

        # Full data table
        with st.expander("üìã All Market Sizing Data"):
            st.dataframe(
                _mkt_df[["metric", "year", "value_b", "source", "notes"]],
                use_container_width=True,
                hide_index=True,
                column_config={
                    "value_b": st.column_config.NumberColumn("Value ($B)", format="$%.1fB"),
                },
            )

        st.info("‚ÑπÔ∏è Market sizing data is manually curated. Update `data/market_sizing.json` with new estimates as they become available.")
    else:
        st.warning("No market sizing data found. Create `data/market_sizing.json` to get started.")

